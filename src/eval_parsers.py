#!/usr/bin/env python3
"""
CLI ê¸°ë°˜ Parser í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸

ë‹¤ì–‘í•œ ì…ë ¥ í¬ë§·ì„ ì§€ì›í•˜ëŠ” Parser ë¹„êµ í…ŒìŠ¤íŠ¸:
1. VLM Parser (Qwen3-VL-2B-Instruct) - ëª¨ë“  í¬ë§· ì§€ì›
2. OCR Parser - Text (pdfplumber) - PDF ì „ìš©
3. OCR Parser - Image (Docling + RapidOCR) - PDF ì „ìš©

ì§€ì› í¬ë§·:
- PDF: ë””ì§€í„¸/ìŠ¤ìº” PDF (ëª¨ë“  íŒŒì„œ í…ŒìŠ¤íŠ¸)
- IMAGE: PNG, JPG, JPEG (VLMë§Œ ì§€ì›)
- HWP/HWPX: í•œê¸€ ë¬¸ì„œ (ì´ë¯¸ì§€ë¡œ ë³€í™˜ í›„ VLM íŒŒì‹±)

Usage:
    # PDF í…ŒìŠ¤íŠ¸
    python -m src.eval_parsers --input data/sample.pdf --gt data/ground_truth.md

    # ì´ë¯¸ì§€ í…ŒìŠ¤íŠ¸ (VLMë§Œ)
    python -m src.eval_parsers --input data/image.png --gt data/ground_truth.md

    # HWP/HWPX í…ŒìŠ¤íŠ¸ (VLMë§Œ, LibreOffice í•„ìš”)
    python -m src.eval_parsers --input data/document.hwp --gt data/ground_truth.md

    # ë ˆê±°ì‹œ (--pdf ì˜µì…˜ë„ ì§€ì›)
    python -m src.eval_parsers --pdf data/sample.pdf
"""

import argparse
import subprocess
import sys
import time
import re
import tempfile
from pathlib import Path
from enum import Enum
from typing import Optional, List

import jiwer

# =============================================================================
# Import Compatibility Layer
# =============================================================================
# ë‘ ê°€ì§€ ì‹¤í–‰ ë°©ì‹ ëª¨ë‘ ì§€ì›:
#   1. python -m src.eval_parsers (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ)
#   2. python eval_parsers.py (src/ ë””ë ‰í† ë¦¬ì—ì„œ)

def _import_parsers():
    """íŒŒì„œ ëª¨ë“ˆì„ ë™ì ìœ¼ë¡œ ì„í¬íŠ¸ (ê²½ë¡œ í˜¸í™˜ì„± ì²˜ë¦¬)"""
    global VLMParser, OCRParser, ImageOCRParser, DoclingParser, check_docling_available

    try:
        # ë°©ë²• 1: src.parsers (í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰)
        from src.parsers.vlm_parser import VLMParser
        from src.parsers.ocr_parser import OCRParser, ImageOCRParser
        from src.parsers.docling_parser import DoclingParser, check_docling_available
    except ImportError:
        try:
            # ë°©ë²• 2: parsers (src/ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰ ë˜ëŠ” PYTHONPATH ì„¤ì •)
            from parsers.vlm_parser import VLMParser
            from parsers.ocr_parser import OCRParser, ImageOCRParser
            from parsers.docling_parser import DoclingParser, check_docling_available
        except ImportError as e:
            print(f"âŒ íŒŒì„œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
            print("   í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”: python -m src.eval_parsers")
            sys.exit(1)

    return VLMParser, OCRParser, ImageOCRParser, DoclingParser, check_docling_available

# ì§€ì—° ì„í¬íŠ¸ë¥¼ ìœ„í•œ í”Œë ˆì´ìŠ¤í™€ë”
VLMParser = None
OCRParser = None
ImageOCRParser = None
DoclingParser = None
check_docling_available = None


# =============================================================================
# File Format Detection
# =============================================================================

class FileFormat(Enum):
    """ì§€ì›í•˜ëŠ” íŒŒì¼ í¬ë§·"""
    PDF = "pdf"
    IMAGE = "image"  # PNG, JPG, JPEG
    HWP = "hwp"      # í•œê¸€ 97-2003
    HWPX = "hwpx"    # í•œê¸€ 2010+
    UNKNOWN = "unknown"


def detect_file_format(file_path: Path) -> FileFormat:
    """íŒŒì¼ í™•ì¥ìë¡œ í¬ë§· ê°ì§€"""
    suffix = file_path.suffix.lower()

    if suffix == ".pdf":
        return FileFormat.PDF
    elif suffix in [".png", ".jpg", ".jpeg", ".webp", ".gif", ".bmp", ".tiff"]:
        return FileFormat.IMAGE
    elif suffix == ".hwp":
        return FileFormat.HWP
    elif suffix == ".hwpx":
        return FileFormat.HWPX
    else:
        return FileFormat.UNKNOWN


def convert_hwp_to_images(hwp_path: Path, dpi: int = 150) -> List[bytes]:
    """HWP/HWPX íŒŒì¼ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜ (LibreOffice ì‚¬ìš©)

    Args:
        hwp_path: HWP/HWPX íŒŒì¼ ê²½ë¡œ
        dpi: ì¶œë ¥ í•´ìƒë„

    Returns:
        PNG ì´ë¯¸ì§€ ë°”ì´íŠ¸ ë¦¬ìŠ¤íŠ¸

    Requirements:
        - LibreOffice ì„¤ì¹˜ í•„ìš” (soffice ëª…ë ¹)
        - Ubuntu: sudo apt install libreoffice
    """
    images = []

    try:
        with tempfile.TemporaryDirectory() as tmpdir:
            tmpdir_path = Path(tmpdir)

            # Step 1: HWP â†’ PDF ë³€í™˜ (LibreOffice)
            print("   HWP â†’ PDF ë³€í™˜ ì¤‘ (LibreOffice)...", end=" ", flush=True)
            result = subprocess.run([
                "soffice",
                "--headless",
                "--convert-to", "pdf",
                "--outdir", str(tmpdir_path),
                str(hwp_path)
            ], capture_output=True, timeout=60)

            if result.returncode != 0:
                print("âœ—")
                print(f"   LibreOffice ì˜¤ë¥˜: {result.stderr.decode()}")
                return []

            print("âœ“")

            # Step 2: ë³€í™˜ëœ PDF ì°¾ê¸°
            pdf_files = list(tmpdir_path.glob("*.pdf"))
            if not pdf_files:
                print("   âŒ PDF íŒŒì¼ ìƒì„± ì‹¤íŒ¨")
                return []

            pdf_path = pdf_files[0]
            pdf_bytes = pdf_path.read_bytes()

            # Step 3: PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
            global ImageOCRParser
            if ImageOCRParser is None:
                _import_parsers()
            image_parser = ImageOCRParser()
            images = image_parser.pdf_to_images(pdf_bytes, dpi=dpi)

            print(f"   PDF â†’ {len(images)} í˜ì´ì§€ ì´ë¯¸ì§€ ë³€í™˜ ì™„ë£Œ")

    except FileNotFoundError:
        print("\n   âŒ LibreOfficeê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   Ubuntu: sudo apt install libreoffice")
        print("   macOS: brew install --cask libreoffice")
    except subprocess.TimeoutExpired:
        print("\n   âŒ ë³€í™˜ íƒ€ì„ì•„ì›ƒ (60ì´ˆ ì´ˆê³¼)")
    except Exception as e:
        print(f"\n   âŒ HWP ë³€í™˜ ì˜¤ë¥˜: {e}")

    return images


# =============================================================================
# Text Normalization (for fair comparison)
# =============================================================================

def normalize_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ê·œí™” - CER/WER ë¹„êµë¥¼ ìœ„í•œ ì „ì²˜ë¦¬

    - ë§ˆí¬ë‹¤ìš´ ë¬¸ë²• ì œê±°
    - ë‹¤ì¤‘ ê³µë°±/ì¤„ë°”ê¿ˆ ì •ê·œí™”
    - ì•ë’¤ ê³µë°± ì œê±°
    """
    if not text:
        return ""

    result = text

    # ë§ˆí¬ë‹¤ìš´ í—¤ë” ì œê±°
    result = re.sub(r'^#{1,6}\s+', '', result, flags=re.MULTILINE)

    # Bold/Italic ì œê±°
    result = re.sub(r'\*\*\*(.+?)\*\*\*', r'\1', result)
    result = re.sub(r'\*\*(.+?)\*\*', r'\1', result)
    result = re.sub(r'\*(.+?)\*', r'\1', result)
    result = re.sub(r'___(.+?)___', r'\1', result)
    result = re.sub(r'__(.+?)__', r'\1', result)
    result = re.sub(r'_(.+?)_', r'\1', result)

    # ë§í¬ ì œê±°
    result = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', result)

    # ë¦¬ìŠ¤íŠ¸ ë§ˆì»¤ ì œê±°
    result = re.sub(r'^[\s]*[-*+]\s+', '', result, flags=re.MULTILINE)
    result = re.sub(r'^[\s]*\d+\.\s+', '', result, flags=re.MULTILINE)

    # í…Œì´ë¸” íŒŒì´í”„ ì œê±°
    result = re.sub(r'\|', ' ', result)

    # ë‹¤ì¤‘ ê³µë°± ì •ê·œí™”
    result = re.sub(r'[ \t]+', ' ', result)
    result = re.sub(r'\n{2,}', '\n', result)

    # ê° ì¤„ ì•ë’¤ ê³µë°± ì œê±°
    lines = [line.strip() for line in result.split('\n')]
    lines = [line for line in lines if line]

    return '\n'.join(lines).strip()


# =============================================================================
# Tokenizer
# =============================================================================

def get_tokenizer(tokenizer_type: str = "whitespace"):
    """í† í¬ë‚˜ì´ì € ë°˜í™˜

    Args:
        tokenizer_type: "whitespace", "mecab", "okt"

    Returns:
        tokenize í•¨ìˆ˜
    """
    if tokenizer_type == "mecab":
        try:
            from konlpy.tag import Mecab
            mecab = Mecab()
            print("âœ“ Mecab í† í¬ë‚˜ì´ì € ì‚¬ìš©")
            return mecab.morphs
        except ImportError:
            print("âš ï¸ konlpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. whitespaceë¡œ fallback")
            return lambda x: x.split()
        except Exception as e:
            print(f"âš ï¸ Mecab ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. whitespaceë¡œ fallback")
            return lambda x: x.split()

    elif tokenizer_type == "okt":
        try:
            from konlpy.tag import Okt
            okt = Okt()
            print("âœ“ Okt í† í¬ë‚˜ì´ì € ì‚¬ìš© (ìˆœìˆ˜ Python, ì‹œìŠ¤í…œ ì˜ì¡´ì„± ì—†ìŒ)")
            return okt.morphs
        except ImportError:
            print("âš ï¸ konlpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. whitespaceë¡œ fallback")
            return lambda x: x.split()
        except Exception as e:
            print(f"âš ï¸ Okt ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. whitespaceë¡œ fallback")
            return lambda x: x.split()

    else:
        return lambda x: x.split()


# =============================================================================
# CER/WER Calculation (using jiwer library)
# =============================================================================


def calculate_cer(hypothesis: str, reference: str) -> dict:
    """Character Error Rate ê³„ì‚° (jiwer ì‚¬ìš©)

    Returns:
        dict with cer, substitutions, deletions, insertions
    """
    if not reference:
        return {"cer": 0.0 if not hypothesis else float('inf')}

    if not hypothesis:
        return {
            "cer": 1.0,
            "substitutions": 0,
            "deletions": len(reference),
            "insertions": 0
        }

    # jiwer.cer: reference first, hypothesis second
    cer_value = jiwer.cer(reference, hypothesis)

    # ìƒì„¸ ì •ë³´
    output = jiwer.process_characters(reference, hypothesis)

    return {
        "cer": cer_value,
        "substitutions": output.substitutions,
        "deletions": output.deletions,
        "insertions": output.insertions,
        "hits": output.hits
    }


def calculate_wer(hypothesis: str, reference: str, tokenizer=None) -> dict:
    """Word Error Rate ê³„ì‚° (jiwer ì‚¬ìš©)

    Args:
        hypothesis: íŒŒì„œ ì¶œë ¥ í…ìŠ¤íŠ¸
        reference: Ground Truth í…ìŠ¤íŠ¸
        tokenizer: í† í¬ë‚˜ì´ì € í•¨ìˆ˜ (Noneì´ë©´ ê³µë°± ë¶„ë¦¬)

    Returns:
        dict with wer, substitutions, deletions, insertions
    """
    def default_tokenizer(x: str) -> list[str]:
        return x.split()

    if tokenizer is None:
        tokenizer = default_tokenizer

    if not reference:
        return {"wer": 0.0 if not hypothesis else float('inf')}

    # í† í°í™”
    ref_tokens = tokenizer(reference)
    hyp_tokens = tokenizer(hypothesis) if hypothesis else []

    if not hyp_tokens:
        return {
            "wer": 1.0,
            "substitutions": 0,
            "deletions": len(ref_tokens),
            "insertions": 0,
            "ref_tokens": len(ref_tokens),
            "hyp_tokens": 0
        }

    # í† í°ì„ ê³µë°±ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ jiwerì— ì „ë‹¬
    ref_str = " ".join(ref_tokens)
    hyp_str = " ".join(hyp_tokens)

    # jiwer.wer: reference first, hypothesis second
    wer_value = jiwer.wer(ref_str, hyp_str)

    # ìƒì„¸ ì •ë³´
    output = jiwer.process_words(ref_str, hyp_str)

    return {
        "wer": wer_value,
        "substitutions": output.substitutions,
        "deletions": output.deletions,
        "insertions": output.insertions,
        "hits": output.hits,
        "ref_tokens": len(ref_tokens),
        "hyp_tokens": len(hyp_tokens)
    }


# =============================================================================
# Parser Tests
# =============================================================================

def test_vlm_parser(
    input_data: bytes,
    verbose: bool = False,
    file_format: FileFormat = FileFormat.PDF,
    pre_converted_images: Optional[List[bytes]] = None
) -> dict:
    """VLM Parser í…ŒìŠ¤íŠ¸ (ë‹¤ì–‘í•œ ì…ë ¥ í¬ë§· ì§€ì›)

    Args:
        input_data: íŒŒì¼ ë°”ì´íŠ¸ ë°ì´í„°
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
        file_format: íŒŒì¼ í¬ë§·
        pre_converted_images: ë¯¸ë¦¬ ë³€í™˜ëœ ì´ë¯¸ì§€ ë¦¬ìŠ¤íŠ¸ (HWPìš©)
    """
    global VLMParser, ImageOCRParser
    if VLMParser is None:
        _import_parsers()

    print("\n" + "=" * 60)
    print("ğŸ¤– VLM Parser (Qwen3-VL-2B-Instruct)")
    print("=" * 60)

    start_time = time.time()

    # ì…ë ¥ í¬ë§·ì— ë”°ë¥¸ ì´ë¯¸ì§€ ì¤€ë¹„
    if pre_converted_images:
        # HWP/HWPX: ë¯¸ë¦¬ ë³€í™˜ëœ ì´ë¯¸ì§€ ì‚¬ìš©
        images = pre_converted_images
        print(f"ğŸ“„ ì…ë ¥: ë¯¸ë¦¬ ë³€í™˜ëœ ì´ë¯¸ì§€ ({len(images)} í˜ì´ì§€)")

    elif file_format == FileFormat.IMAGE:
        # ì´ë¯¸ì§€ íŒŒì¼: ë°”ë¡œ ì‚¬ìš©
        images = [input_data]
        print("ğŸ“„ ì…ë ¥: ë‹¨ì¼ ì´ë¯¸ì§€ íŒŒì¼")

    elif file_format == FileFormat.PDF:
        # PDF â†’ ì´ë¯¸ì§€ ë³€í™˜
        image_parser = ImageOCRParser()
        images = image_parser.pdf_to_images(input_data, dpi=150)

        if not images:
            print("âŒ PDF â†’ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨")
            return {"success": False, "error": "ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨"}

        print(f"ğŸ“„ í˜ì´ì§€ ìˆ˜: {len(images)}")

    else:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” í¬ë§·: {file_format}")
        return {"success": False, "error": f"Unsupported format: {file_format}"}

    # VLM íŒŒì‹±
    vlm_parser = VLMParser()
    results = []

    for i, img_bytes in enumerate(images):
        print(f"  ì²˜ë¦¬ ì¤‘: Page {i+1}/{len(images)}...", end=" ", flush=True)
        result = vlm_parser.parse(img_bytes)
        results.append(result)

        if result.success:
            print(f"âœ“ ({result.elapsed_time:.2f}s)")
        else:
            print(f"âœ— ({result.error})")

    total_time = time.time() - start_time

    # ê²°ê³¼ í•©ì¹˜ê¸°
    combined_content = "\n\n".join(
        r.content for r in results if r.success and r.content
    )

    success_count = sum(1 for r in results if r.success)

    print("\nğŸ“Š ê²°ê³¼:")
    print(f"   - ì„±ê³µ: {success_count}/{len(results)} í˜ì´ì§€")
    print(f"   - ì´ ì‹œê°„: {total_time:.2f}s")
    print(f"   - í‰ê· : {total_time/len(images):.2f}s/page")
    print(f"   - ì¶”ì¶œ ê¸¸ì´: {len(combined_content)} chars")

    if verbose and combined_content:
        print("\nğŸ“ ì¶”ì¶œ ê²°ê³¼ (ì²˜ìŒ 500ì):")
        print("-" * 40)
        print(combined_content[:500])
        print("-" * 40)

    return {
        "success": success_count > 0,
        "content": combined_content,
        "elapsed_time": total_time,
        "page_count": len(images),
        "per_page_time": total_time / len(images)
    }


def test_ocr_text_parser(pdf_bytes: bytes, verbose: bool = False) -> dict:
    """OCR Parser (Text - pdfplumber) í…ŒìŠ¤íŠ¸"""
    global OCRParser
    if OCRParser is None:
        _import_parsers()

    print("\n" + "=" * 60)
    print("ğŸ“– OCR Parser - Text (pdfplumber)")
    print("=" * 60)

    parser = OCRParser()

    # PDF íƒ€ì… í™•ì¸
    pdf_type = parser.detect_pdf_type(pdf_bytes)
    print(f"ğŸ“„ PDF íƒ€ì…: {pdf_type}")

    # íŒŒì‹±
    result = parser.parse_pdf(pdf_bytes)

    print("\nğŸ“Š ê²°ê³¼:")
    print(f"   - ì„±ê³µ: {'âœ“' if result.success else 'âœ—'}")
    print(f"   - í˜ì´ì§€ ìˆ˜: {result.page_count}")
    print(f"   - í…ìŠ¤íŠ¸ ì¡´ì¬: {'âœ“' if result.has_text else 'âœ—'}")
    print(f"   - í‘œ ê°œìˆ˜: {len(result.tables)}")
    print(f"   - ì´ ì‹œê°„: {result.elapsed_time:.2f}s")
    print(f"   - ì¶”ì¶œ ê¸¸ì´: {len(result.content)} chars")

    if verbose and result.content:
        print("\nğŸ“ ì¶”ì¶œ ê²°ê³¼ (ì²˜ìŒ 500ì):")
        print("-" * 40)
        print(result.content[:500])
        print("-" * 40)

    return {
        "success": result.success,
        "content": result.content,
        "elapsed_time": result.elapsed_time,
        "page_count": result.page_count,
        "has_text": result.has_text,
        "pdf_type": pdf_type
    }


def test_ocr_image_parser(pdf_bytes: bytes, verbose: bool = False) -> dict:
    """OCR Parser (Image - Docling) í…ŒìŠ¤íŠ¸"""
    global DoclingParser, check_docling_available
    if DoclingParser is None:
        _import_parsers()

    print("\n" + "=" * 60)
    print("ğŸ” OCR Parser - Image (Docling + RapidOCR)")
    print("=" * 60)

    if not check_docling_available():
        print("âŒ Docling ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   pip install docling ëª…ë ¹ìœ¼ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
        return {"success": False, "error": "Docling not installed"}

    parser = DoclingParser(ocr_enabled=True)
    result = parser.parse_pdf(pdf_bytes)

    print("\nğŸ“Š ê²°ê³¼:")
    print(f"   - ì„±ê³µ: {'âœ“' if result.success else 'âœ—'}")
    print(f"   - í˜ì´ì§€ ìˆ˜: {result.page_count}")
    print(f"   - ì´ ì‹œê°„: {result.elapsed_time:.2f}s")
    print(f"   - ì¶”ì¶œ ê¸¸ì´ (text): {len(result.content)} chars")
    print(f"   - ì¶”ì¶œ ê¸¸ì´ (markdown): {len(result.markdown)} chars")

    if result.error:
        print(f"   - ì—ëŸ¬: {result.error}")

    if verbose and result.content:
        print("\nğŸ“ ì¶”ì¶œ ê²°ê³¼ (ì²˜ìŒ 500ì):")
        print("-" * 40)
        print(result.content[:500])
        print("-" * 40)

    return {
        "success": result.success,
        "content": result.content,
        "markdown": result.markdown,
        "elapsed_time": result.elapsed_time,
        "page_count": result.page_count,
        "error": result.error
    }


# =============================================================================
# Evaluation
# =============================================================================

def evaluate_results(results: dict, ground_truth: str, tokenizer=None, tokenizer_name: str = "whitespace") -> dict:
    """ê²°ê³¼ í‰ê°€ (CER, WER ê³„ì‚°) - jiwer ì‚¬ìš©

    Args:
        results: íŒŒì„œë³„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        ground_truth: Ground Truth í…ìŠ¤íŠ¸
        tokenizer: WER ê³„ì‚°ìš© í† í¬ë‚˜ì´ì € í•¨ìˆ˜
        tokenizer_name: í† í¬ë‚˜ì´ì € ì´ë¦„ (ì¶œë ¥ìš©)
    """
    print("\n" + "=" * 60)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ (Ground Truth ë¹„êµ) - jiwer")
    print(f"   WER Tokenizer: {tokenizer_name}")
    print("=" * 60)

    gt_normalized = normalize_text(ground_truth)
    print(f"Ground Truth ê¸¸ì´: {len(gt_normalized)} chars (ì •ê·œí™” í›„)")
    print()

    evaluation = {}

    for parser_name, result in results.items():
        if not result.get("success"):
            print(f"{parser_name}: SKIP (íŒŒì‹± ì‹¤íŒ¨)")
            evaluation[parser_name] = {"cer": None, "wer": None}
            continue

        content = result.get("content", "")
        if not content:
            print(f"{parser_name}: SKIP (ë‚´ìš© ì—†ìŒ)")
            evaluation[parser_name] = {"cer": None, "wer": None}
            continue

        # ì •ê·œí™”
        content_normalized = normalize_text(content)

        # CER, WER ê³„ì‚° (jiwer)
        cer_result = calculate_cer(content_normalized, gt_normalized)
        wer_result = calculate_wer(content_normalized, gt_normalized, tokenizer)

        cer = cer_result["cer"]
        wer = wer_result["wer"]

        print(f"{parser_name}:")
        print(f"   - CER: {cer:.4f} ({cer*100:.2f}%)")
        print(f"      â””â”€ S:{cer_result.get('substitutions', 0)} D:{cer_result.get('deletions', 0)} I:{cer_result.get('insertions', 0)}")
        print(f"   - WER: {wer:.4f} ({wer*100:.2f}%)")
        print(f"      â””â”€ S:{wer_result.get('substitutions', 0)} D:{wer_result.get('deletions', 0)} I:{wer_result.get('insertions', 0)}")
        print(f"      â””â”€ Tokens: ref={wer_result.get('ref_tokens', 0)} hyp={wer_result.get('hyp_tokens', 0)}")
        print(f"   - Latency: {result.get('elapsed_time', 0):.2f}s")
        print()

        evaluation[parser_name] = {
            "cer": cer,
            "cer_detail": cer_result,
            "wer": wer,
            "wer_detail": wer_result,
            "latency": result.get("elapsed_time", 0),
            "tokenizer": tokenizer_name
        }

    return evaluation


def save_results_to_files(results: dict, output_dir: str, pdf_name: str, evaluation: dict = None, tokenizer_name: str = "whitespace", metadata: dict = None):
    """íŒŒì‹± ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥

    Args:
        results: íŒŒì„œë³„ ê²°ê³¼
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        pdf_name: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
        evaluation: í‰ê°€ ê²°ê³¼ (CER, WER)
        tokenizer_name: í† í¬ë‚˜ì´ì € ì´ë¦„
        metadata: í…ŒìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° (title, description, doc_type ë“±)
    """
    from pathlib import Path
    import json
    from datetime import datetime

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    _ = Path(pdf_name).stem  # base_name reserved for future use

    print("\n" + "=" * 60)
    print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥: {output_path}")
    print("=" * 60)

    saved_files = []

    # 1. ê° íŒŒì„œë³„ ì¶œë ¥ ì €ì¥
    for parser_name, result in results.items():
        if not result.get("success"):
            continue

        content = result.get("content", "")
        if not content:
            continue

        safe_name = parser_name.lower().replace(" ", "-")
        ext = ".md" if "markdown" in result else ".txt"
        filename = f"{safe_name}_output{ext}"
        filepath = output_path / filename

        save_content = result.get("markdown", content)
        filepath.write_text(save_content, encoding="utf-8")
        saved_files.append(filepath)
        print(f"   âœ“ {filename} ({len(save_content)} chars)")

    # 2. í‰ê°€ ê²°ê³¼ JSON ì €ì¥
    eval_json = {
        "pdf": pdf_name,
        "timestamp": timestamp,
        "tokenizer": tokenizer_name,
        "results": {}
    }

    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
    if metadata:
        eval_json["metadata"] = {
            "title": metadata.get("title", ""),
            "description": metadata.get("description", ""),
            "doc_type": metadata.get("doc_type", "unknown"),
            "language": metadata.get("language", "unknown"),
            "tags": metadata.get("tags", [])
        }

    for name, result in results.items():
        eval_json["results"][name] = {
            "success": result.get("success"),
            "elapsed_time": result.get("elapsed_time"),
            "content_length": len(result.get("content", ""))
        }
        if evaluation and name in evaluation:
            eval_data = evaluation[name]
            eval_json["results"][name]["cer"] = eval_data.get("cer")
            eval_json["results"][name]["wer"] = eval_data.get("wer")

    meta_path = output_path / "evaluation.json"
    meta_path.write_text(json.dumps(eval_json, indent=2, ensure_ascii=False), encoding="utf-8")
    print("   âœ“ evaluation.json")

    # 3. ìš”ì•½ ë§ˆí¬ë‹¤ìš´ ì €ì¥
    summary_lines = [
        "# Parsing Test Results",
        "",
        f"- **Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        f"- **PDF**: {pdf_name}",
        f"- **Tokenizer**: {tokenizer_name}",
        "",
        "## Results",
        "",
        "| Parser | Success | Latency | Length |",
        "|--------|---------|---------|--------|",
    ]

    for name, result in results.items():
        success = "âœ“" if result.get("success") else "âœ—"
        latency = f"{result.get('elapsed_time', 0):.2f}s"
        length = f"{len(result.get('content', ''))} chars"
        summary_lines.append(f"| {name} | {success} | {latency} | {length} |")

    if evaluation:
        summary_lines.extend([
            "",
            "## Evaluation (vs Ground Truth)",
            "",
            "| Parser | CER | WER |",
            "|--------|-----|-----|",
        ])
        for name, eval_data in evaluation.items():
            cer = eval_data.get("cer")
            wer = eval_data.get("wer")
            cer_str = f"{cer*100:.2f}%" if cer is not None else "N/A"
            wer_str = f"{wer*100:.2f}%" if wer is not None else "N/A"
            summary_lines.append(f"| {name} | {cer_str} | {wer_str} |")

    summary_path = output_path / "README.md"
    summary_path.write_text("\n".join(summary_lines), encoding="utf-8")
    print("   âœ“ README.md (ìš”ì•½)")

    return saved_files


def print_summary(results: dict, evaluation: dict = None):
    """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "=" * 60)
    print("ğŸ“‹ í…ŒìŠ¤íŠ¸ ìš”ì•½")
    print("=" * 60)

    print("\n| Parser | ì„±ê³µ | ì‹œê°„ | ì¶”ì¶œ ê¸¸ì´ |")
    print("|--------|------|------|----------|")

    for name, result in results.items():
        success = "âœ“" if result.get("success") else "âœ—"
        time_str = f"{result.get('elapsed_time', 0):.2f}s"
        length = len(result.get("content", ""))
        print(f"| {name} | {success} | {time_str} | {length} chars |")

    if evaluation:
        print("\n| Parser | CER | WER | Latency |")
        print("|--------|-----|-----|---------|")

        for name, eval_result in evaluation.items():
            cer = eval_result.get("cer")
            wer = eval_result.get("wer")
            latency = eval_result.get("latency", 0)

            cer_str = f"{cer*100:.2f}%" if cer is not None else "N/A"
            wer_str = f"{wer*100:.2f}%" if wer is not None else "N/A"

            print(f"| {name} | {cer_str} | {wer_str} | {latency:.2f}s |")


# =============================================================================
# Data Folder Scanning
# =============================================================================

def extract_file_metadata(file_path: Path) -> dict:
    """íŒŒì¼ì—ì„œ ìë™ìœ¼ë¡œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ

    Args:
        file_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ

    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    import os

    metadata = {
        "filename": file_path.name,
        "file_size_kb": round(file_path.stat().st_size / 1024, 1),
        "doc_type": "unknown",
        "pages": 0,
        "title": file_path.stem,  # í™•ì¥ì ì œì™¸í•œ íŒŒì¼ëª…
        "language": "unknown",
        "has_text_layer": False,
    }

    # íŒŒì¼ í¬ë§· ê°ì§€
    suffix = file_path.suffix.lower()
    if suffix == ".pdf":
        metadata["doc_type"] = "PDF"
        try:
            import pdfplumber
            with pdfplumber.open(file_path) as pdf:
                metadata["pages"] = len(pdf.pages)

                # í…ìŠ¤íŠ¸ ë ˆì´ì–´ í™•ì¸ (ì²« í˜ì´ì§€)
                if pdf.pages:
                    first_page_text = pdf.pages[0].extract_text() or ""
                    # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ê°€ 100ì ì´ìƒì´ë©´ í…ìŠ¤íŠ¸ ë ˆì´ì–´ ìˆìŒ
                    metadata["has_text_layer"] = len(first_page_text.strip()) > 100

        except Exception as e:
            print(f"âš ï¸ PDF ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    elif suffix in [".jpg", ".jpeg", ".png", ".gif", ".bmp", ".webp"]:
        metadata["doc_type"] = "Image"
        metadata["pages"] = 1
        try:
            from PIL import Image
            with Image.open(file_path) as img:
                metadata["image_size"] = f"{img.width}x{img.height}"
        except Exception:
            pass

    elif suffix in [".hwp", ".hwpx"]:
        metadata["doc_type"] = "HWP"
        # HWP ë©”íƒ€ë°ì´í„° ì¶”ì¶œì€ ë³µì¡í•˜ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©

    # ì–¸ì–´ ê°ì§€ (íŒŒì¼ëª… ê¸°ë°˜ ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    filename = file_path.name
    if any(ord(c) >= 0xAC00 and ord(c) <= 0xD7A3 for c in filename):
        metadata["language"] = "ko"
    elif any(ord(c) >= 0x4E00 and ord(c) <= 0x9FFF for c in filename):
        metadata["language"] = "zh"
    elif any(ord(c) >= 0x3040 and ord(c) <= 0x30FF for c in filename):
        metadata["language"] = "ja"
    else:
        metadata["language"] = "en"  # ê¸°ë³¸ê°’

    return metadata


def load_test_metadata(folder_path: Path, input_file: Optional[Path] = None) -> dict:
    """í…ŒìŠ¤íŠ¸ í´ë”ì˜ ë©”íƒ€ë°ì´í„° ë¡œë“œ (íŒŒì¼ì—ì„œ ìë™ ì¶”ì¶œ)

    Args:
        folder_path: data/test_* í´ë” ê²½ë¡œ
        input_file: ì…ë ¥ íŒŒì¼ ê²½ë¡œ (ìˆìœ¼ë©´ íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ)

    Returns:
        ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    # ì…ë ¥ íŒŒì¼ì´ ìˆìœ¼ë©´ ìë™ ì¶”ì¶œ
    if input_file and input_file.exists():
        return extract_file_metadata(input_file)

    # í´ë”ì—ì„œ ì…ë ¥ íŒŒì¼ ì°¾ê¸°
    input_extensions = [".pdf", ".png", ".jpg", ".jpeg", ".hwp", ".hwpx"]
    for ext in input_extensions:
        for f in folder_path.glob(f"*{ext}"):
            if not f.name.startswith("gt_"):
                return extract_file_metadata(f)

    # ê¸°ë³¸ê°’ ë°˜í™˜
    return {
        "filename": "unknown",
        "file_size_kb": 0,
        "doc_type": "unknown",
        "pages": 0,
        "title": folder_path.name,
        "language": "unknown",
        "has_text_layer": False,
    }


def scan_data_folders(data_dir: Path = Path("data")) -> List[dict]:
    """data/ í´ë”ì˜ ëª¨ë“  test_* í´ë”ë¥¼ ìŠ¤ìº”í•˜ì—¬ í…ŒìŠ¤íŠ¸ ì •ë³´ ë°˜í™˜

    Returns:
        List of dicts with keys: test_id, input_file, gt_file, folder_path, metadata
    """
    test_folders = []

    if not data_dir.exists():
        print(f"âŒ ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return []

    # test_* íŒ¨í„´ì˜ í´ë” ì°¾ê¸°
    for folder in sorted(data_dir.iterdir()):
        if not folder.is_dir() or not folder.name.startswith("test_"):
            continue

        test_id = folder.name
        input_file = None
        gt_file = None

        # ì…ë ¥ íŒŒì¼ ì°¾ê¸° (PDF, ì´ë¯¸ì§€, HWP ë“±)
        for f in folder.iterdir():
            if f.is_file():
                fmt = detect_file_format(f)
                if fmt != FileFormat.UNKNOWN and not f.name.startswith("gt_"):
                    input_file = f
                elif f.name.startswith("gt_") and f.suffix in [".md", ".txt"]:
                    gt_file = f

        if input_file:
            # íŒŒì¼ì—ì„œ ë©”íƒ€ë°ì´í„° ìë™ ì¶”ì¶œ
            metadata = load_test_metadata(folder, input_file)

            test_folders.append({
                "test_id": test_id,
                "input_file": input_file,
                "gt_file": gt_file,
                "folder_path": folder,
                "metadata": metadata
            })
        else:
            print(f"âš ï¸ {test_id}: ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

    return test_folders


def run_single_test(
    input_path: Path,
    gt_path: Optional[Path] = None,
    output_dir: Optional[Path] = None,
    skip_vlm: bool = False,
    skip_docling: bool = False,
    verbose: bool = False,
    tokenizer_type: str = "whitespace",
    dpi: int = 150,
    metadata: dict = None
) -> dict:
    """ë‹¨ì¼ íŒŒì¼ íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹¤í–‰

    Args:
        metadata: í…ŒìŠ¤íŠ¸ ë©”íƒ€ë°ì´í„° (title, description, doc_type ë“±)

    Returns:
        dict with keys: results, evaluation
    """
    file_format = detect_file_format(input_path)
    input_bytes = input_path.read_bytes()

    print("=" * 60)
    print("ğŸ”¬ VLM Document Parsing Quality Test")
    print("=" * 60)
    print(f"ğŸ“„ ì…ë ¥ íŒŒì¼: {input_path}")
    print(f"ğŸ“ í¬ë§·: {file_format.value.upper()}")
    print(f"ğŸ“¦ í¬ê¸°: {len(input_bytes) / 1024:.1f} KB")

    if file_format == FileFormat.UNKNOWN:
        print("âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í¬ë§·ì…ë‹ˆë‹¤.")
        return {"results": {}, "evaluation": None}

    # Ground Truth ì½ê¸°
    ground_truth = None
    if gt_path and gt_path.exists():
        ground_truth = gt_path.read_text(encoding="utf-8")
        print(f"ğŸ“‹ Ground Truth: {gt_path} ({len(ground_truth)} chars)")

    results = {}

    # HWP/HWPX ì „ì²˜ë¦¬
    hwp_images = None
    if file_format in [FileFormat.HWP, FileFormat.HWPX]:
        print("\nğŸ“„ HWP/HWPX ë³€í™˜ ì‹œì‘...")
        hwp_images = convert_hwp_to_images(input_path, dpi=dpi)
        if not hwp_images:
            print("âŒ HWP â†’ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨")
            return {"results": {}, "evaluation": None}

    # í¬ë§·ë³„ í…ŒìŠ¤íŠ¸
    if file_format == FileFormat.PDF:
        if not skip_vlm:
            try:
                results["VLM"] = test_vlm_parser(input_bytes, verbose, FileFormat.PDF)
            except Exception as e:
                print(f"âŒ VLM Parser ì˜¤ë¥˜: {e}")
                results["VLM"] = {"success": False, "error": str(e)}

        try:
            results["OCR-Text"] = test_ocr_text_parser(input_bytes, verbose)
        except Exception as e:
            print(f"âŒ OCR-Text Parser ì˜¤ë¥˜: {e}")
            results["OCR-Text"] = {"success": False, "error": str(e)}

        if not skip_docling:
            try:
                results["OCR-Image"] = test_ocr_image_parser(input_bytes, verbose)
            except Exception as e:
                print(f"âŒ OCR-Image Parser ì˜¤ë¥˜: {e}")
                results["OCR-Image"] = {"success": False, "error": str(e)}

    elif file_format == FileFormat.IMAGE:
        print("\nâš ï¸ ì´ë¯¸ì§€ ì…ë ¥: VLM Parserë§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤")
        if not skip_vlm:
            try:
                results["VLM"] = test_vlm_parser(input_bytes, verbose, FileFormat.IMAGE)
            except Exception as e:
                print(f"âŒ VLM Parser ì˜¤ë¥˜: {e}")
                results["VLM"] = {"success": False, "error": str(e)}

    elif file_format in [FileFormat.HWP, FileFormat.HWPX]:
        print("\nâš ï¸ HWP/HWPX ì…ë ¥: VLM Parserë§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤")
        if not skip_vlm:
            try:
                results["VLM"] = test_vlm_parser(
                    input_bytes, verbose, file_format, pre_converted_images=hwp_images
                )
            except Exception as e:
                print(f"âŒ VLM Parser ì˜¤ë¥˜: {e}")
                results["VLM"] = {"success": False, "error": str(e)}

    # í‰ê°€
    evaluation = None
    if ground_truth:
        tokenizer = get_tokenizer(tokenizer_type)
        evaluation = evaluate_results(results, ground_truth, tokenizer, tokenizer_type)

    # ê²°ê³¼ ì €ì¥
    if output_dir:
        save_results_to_files(results, str(output_dir), str(input_path), evaluation, tokenizer_type, metadata)

    print_summary(results, evaluation)

    return {"results": results, "evaluation": evaluation}


def run_all_tests(
    data_dir: Path = Path("data"),
    results_dir: Path = Path("results"),
    skip_vlm: bool = False,
    skip_docling: bool = False,
    verbose: bool = False,
    tokenizer_type: str = "whitespace",
    dpi: int = 150
) -> dict:
    """data/ í´ë”ì˜ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰

    Returns:
        dict mapping test_id to test results
    """
    test_folders = scan_data_folders(data_dir)

    if not test_folders:
        print("âŒ í…ŒìŠ¤íŠ¸í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return {}

    print("=" * 60)
    print("ğŸ”¬ VLM Document Parsing - Batch Test")
    print("=" * 60)
    print(f"ğŸ“ ë°ì´í„° í´ë”: {data_dir}")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ìˆ˜: {len(test_folders)}")
    print()

    for info in test_folders:
        fmt = detect_file_format(info["input_file"])
        gt_status = "âœ“" if info["gt_file"] else "âœ—"
        print(f"  - {info['test_id']}: {info['input_file'].name} ({fmt.value}) [GT: {gt_status}]")

    print()

    all_results = {}

    for i, info in enumerate(test_folders, 1):
        test_id = info["test_id"]
        print()
        print("#" * 60)
        print(f"# [{i}/{len(test_folders)}] {test_id}")
        print("#" * 60)

        output_dir = results_dir / test_id

        result = run_single_test(
            input_path=info["input_file"],
            gt_path=info["gt_file"],
            output_dir=output_dir,
            skip_vlm=skip_vlm,
            skip_docling=skip_docling,
            verbose=verbose,
            tokenizer_type=tokenizer_type,
            dpi=dpi,
            metadata=info.get("metadata")
        )

        all_results[test_id] = result

    # ì „ì²´ ìš”ì•½
    print()
    print("=" * 60)
    print("ğŸ“Š ì „ì²´ í…ŒìŠ¤íŠ¸ ìš”ì•½")
    print("=" * 60)

    print(f"\n| Test ID | Format | VLM CER | OCR-Text CER | OCR-Image CER |")
    print(f"|---------|--------|---------|--------------|---------------|")

    for test_id, result in all_results.items():
        eval_data = result.get("evaluation", {}) or {}

        vlm_cer = eval_data.get("VLM", {}).get("cer")
        ocr_text_cer = eval_data.get("OCR-Text", {}).get("cer")
        ocr_image_cer = eval_data.get("OCR-Image", {}).get("cer")

        vlm_str = f"{vlm_cer*100:.1f}%" if vlm_cer is not None else "N/A"
        ocr_text_str = f"{ocr_text_cer*100:.1f}%" if ocr_text_cer is not None else "N/A"
        ocr_image_str = f"{ocr_image_cer*100:.1f}%" if ocr_image_cer is not None else "N/A"

        # í¬ë§· ì •ë³´
        test_info = next((t for t in test_folders if t["test_id"] == test_id), None)
        fmt = detect_file_format(test_info["input_file"]).value if test_info else "?"

        print(f"| {test_id:<8} | {fmt:<6} | {vlm_str:<7} | {ocr_text_str:<12} | {ocr_image_str:<13} |")

    return all_results


# =============================================================================
# Main
# =============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="ë‹¤ì¤‘ í¬ë§· Parser ë¹„êµ í…ŒìŠ¤íŠ¸ (VLM, OCR-Text, OCR-Image)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì§€ì› í¬ë§·:
  PDF       : ë””ì§€í„¸/ìŠ¤ìº” PDF (ëª¨ë“  íŒŒì„œ í…ŒìŠ¤íŠ¸)
  IMAGE     : PNG, JPG, JPEG, WebP, GIF, BMP, TIFF (VLMë§Œ)
  HWP/HWPX  : í•œê¸€ ë¬¸ì„œ (LibreOfficeë¡œ ë³€í™˜ í›„ VLM)

ì˜ˆì‹œ:
  # ì „ì²´ í…ŒìŠ¤íŠ¸ (data/ í´ë”ì˜ ëª¨ë“  test_* ìŠ¤ìº”)
  python -m src.eval_parsers --all

  # ë‹¨ì¼ íŒŒì¼ í…ŒìŠ¤íŠ¸
  python -m src.eval_parsers --input data/test_1/test.pdf --gt data/test_1/gt.md

  # íŠ¹ì • ë°ì´í„° í´ë” ì§€ì •
  python -m src.eval_parsers --all --data-dir ./my_data
        """
    )

    # ì…ë ¥ ëª¨ë“œ (--all ë˜ëŠ” --input)
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument(
        "--all", "-a",
        action="store_true",
        help="data/ í´ë”ì˜ ëª¨ë“  test_* í´ë” í…ŒìŠ¤íŠ¸"
    )
    input_group.add_argument(
        "--input", "-i",
        help="í…ŒìŠ¤íŠ¸í•  ì…ë ¥ íŒŒì¼ (PDF, ì´ë¯¸ì§€, HWP/HWPX)"
    )
    input_group.add_argument(
        "--pdf", "-p",
        help="í…ŒìŠ¤íŠ¸í•  PDF íŒŒì¼ ê²½ë¡œ (ë ˆê±°ì‹œ ì˜µì…˜, --input ì‚¬ìš© ê¶Œì¥)"
    )

    parser.add_argument(
        "--data-dir",
        type=Path,
        default=Path("data"),
        help="í…ŒìŠ¤íŠ¸ ë°ì´í„° í´ë” (ê¸°ë³¸: data/)"
    )
    parser.add_argument(
        "--results-dir",
        type=Path,
        default=Path("results"),
        help="ê²°ê³¼ ì €ì¥ í´ë” (ê¸°ë³¸: results/)"
    )
    parser.add_argument(
        "--gt", "-g",
        help="Ground Truth íŒŒì¼ ê²½ë¡œ (--input ì‚¬ìš© ì‹œ)"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="ìƒì„¸ ì¶œë ¥ (ì¶”ì¶œ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°)"
    )
    parser.add_argument(
        "--skip-vlm",
        action="store_true",
        help="VLM Parser í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ"
    )
    parser.add_argument(
        "--skip-docling",
        action="store_true",
        help="Docling Parser í…ŒìŠ¤íŠ¸ ìŠ¤í‚µ"
    )
    parser.add_argument(
        "--output-dir", "-o",
        help="íŒŒì‹± ê²°ê³¼ë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ (--input ì‚¬ìš© ì‹œ)"
    )
    parser.add_argument(
        "--tokenizer", "-t",
        choices=["whitespace", "mecab", "okt"],
        default="whitespace",
        help="WER ê³„ì‚°ìš© í† í¬ë‚˜ì´ì € (ê¸°ë³¸: whitespace)"
    )
    parser.add_argument(
        "--dpi",
        type=int,
        default=150,
        help="PDF/HWP â†’ ì´ë¯¸ì§€ ë³€í™˜ í•´ìƒë„ (ê¸°ë³¸: 150)"
    )

    args = parser.parse_args()

    # --all ëª¨ë“œ: ì „ì²´ í…ŒìŠ¤íŠ¸
    if args.all:
        run_all_tests(
            data_dir=args.data_dir,
            results_dir=args.results_dir,
            skip_vlm=args.skip_vlm,
            skip_docling=args.skip_docling,
            verbose=args.verbose,
            tokenizer_type=args.tokenizer,
            dpi=args.dpi
        )
        return

    # ë‹¨ì¼ íŒŒì¼ ëª¨ë“œ
    input_path = Path(args.input if args.input else args.pdf)
    if not input_path.exists():
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}")
        sys.exit(1)

    gt_path = Path(args.gt) if args.gt else None
    output_dir = Path(args.output_dir) if args.output_dir else None

    run_single_test(
        input_path=input_path,
        gt_path=gt_path,
        output_dir=output_dir,
        skip_vlm=args.skip_vlm,
        skip_docling=args.skip_docling,
        verbose=args.verbose,
        tokenizer_type=args.tokenizer,
        dpi=args.dpi
    )


if __name__ == "__main__":
    main()
