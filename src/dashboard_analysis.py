"""
VLM Document Parsing Quality Analysis Dashboard

CLI í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ì—¬ Tech Report ì‘ì„±ì„ ì§€ì›í•˜ëŠ” ì •ì  ëŒ€ì‹œë³´ë“œ

Features:
- JSON ê²°ê³¼ íŒŒì¼ ë¡œë“œ (results/parsing_results.json)
- @st.cache_data ìºì‹± (1ì‹œê°„ TTL)
- í˜ì´ì§€ë„¤ì´ì…˜ (10ê°œ í…ŒìŠ¤íŠ¸ ì´ˆê³¼ ì‹œ)
- ì°¨íŠ¸ PNG ë‹¤ìš´ë¡œë“œ
- CSV ë‚´ë³´ë‚´ê¸°

Usage:
    streamlit run src/dashboard_analysis.py
"""

import sys
from pathlib import Path

_src_dir = Path(__file__).parent
if str(_src_dir) not in sys.path:
    sys.path.insert(0, str(_src_dir))

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from typing import Dict, List, Any
import numpy as np

from dashboard.data_loader import (
    load_all_results_cached,
    get_test_ids,
    get_parser_names,
    get_parsing_summary_df,
    get_chunking_summary_df,
    get_aggregated_parser_df,
    get_test_evaluation,
    get_test_chunking,
    get_chunking_for_test,
    get_tests_with_chunking,
    get_chart_download_config,
    export_df_to_csv,
    # Backward compatibility
    get_parsing_data,
    get_chunking_data,
    paginate_data,
    get_chunking_parsers,
    get_chunking_data_for_parser,
)
from dashboard.charts import (
    STRATEGY_COLORS,
    create_parser_chunking_comparison,
    create_bc_document_flow,
    create_cs_mean_std_bar,
)
from dashboard.styles import PARSER_COLORS as STYLE_PARSER_COLORS

# =============================================================================
# í˜ì´ì§€ ì„¤ì •
# =============================================================================

st.set_page_config(
    page_title="VLM Document Parsing Quality Analysis",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="collapsed",
)

# =============================================================================
# ìŠ¤íƒ€ì¼ ì„¤ì •
# =============================================================================

st.markdown("""
<style>
    /* Sidebar ì™„ì „ ìˆ¨ê¹€ */
    [data-testid="stSidebar"] { display: none; }
    [data-testid="stSidebarCollapsedControl"] { display: none; }

    /* ì „ì²´ ë°°ê²½ */
    .stApp { background-color: #FAFAFA; }

    /* í—¤ë” */
    h1, h2, h3 { color: #1a1a2e !important; font-weight: 600 !important; }

    /* ë©”íŠ¸ë¦­ ì¹´ë“œ */
    [data-testid="stMetric"] {
        background-color: #FFFFFF;
        padding: 1rem;
        border-radius: 8px;
        border: 1px solid #E5E5E5;
    }
    [data-testid="stMetricValue"] { color: #1a1a2e !important; font-size: 1.5rem !important; }
    [data-testid="stMetricLabel"] { color: #666666 !important; }

    /* íƒ­ */
    .stTabs [data-baseweb="tab-list"] { gap: 8px; border-bottom: 1px solid #E5E5E5; }
    .stTabs [data-baseweb="tab"] {
        color: #666666;
        font-weight: 500;
        padding: 0.75rem 1.5rem;
    }
    .stTabs [aria-selected="true"] {
        color: #1a1a2e !important;
        border-bottom: 2px solid #4F46E5 !important;
    }

    /* í…Œì´ë¸” */
    .stDataFrame { border-radius: 8px; }

    /* êµ¬ë¶„ì„  */
    hr { border-color: #E5E5E5; margin: 2rem 0; }

    /* ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ */
    .download-btn {
        background-color: #F3F4F6;
        border: 1px solid #E5E5E5;
        border-radius: 6px;
        padding: 0.5rem 1rem;
        font-size: 0.875rem;
        cursor: pointer;
    }
</style>
""", unsafe_allow_html=True)

# =============================================================================
# ìƒìˆ˜
# =============================================================================

VERSION = "v0.4.0"  # Added parser-specific chunking analysis (MoC-based)
PAGE_SIZE = 10  # í˜ì´ì§€ë„¤ì´ì…˜ í¬ê¸°

# ë™ì  ìƒ‰ìƒ ìƒì„±ìš© ê¸°ë³¸ íŒ”ë ˆíŠ¸ (íŒŒì„œ ì¶”ê°€ ì‹œ ìë™ í™•ì¥)
DEFAULT_COLORS = [
    "#4F46E5",  # VLM - ì¸ë””ê³ 
    "#059669",  # OCR-Text - ì—ë©”ë„ë“œ
    "#D97706",  # OCR-Image - ì•°ë²„
    "#7C3AED",  # TwoStage-Text - ë³´ë¼
    "#0891B2",  # TwoStage-Image - ì²­ë¡
    "#DC2626",  # ì—¬ìœ  - ë ˆë“œ
    "#EC4899",  # ì—¬ìœ  - í•‘í¬
]


def get_parser_colors(parsers: List[str]) -> Dict[str, str]:
    """íŒŒì„œë³„ ìƒ‰ìƒ ë™ì  ìƒì„± (styles.pyì˜ PARSER_COLORSë¥¼ ë‹¨ì¼ ì§„ì‹¤ ê³µê¸‰ì›ìœ¼ë¡œ ì‚¬ìš©)"""
    colors = {}
    for i, parser in enumerate(parsers):
        # styles.pyì—ì„œ ì •ì˜ëœ ìƒ‰ìƒ ì‚¬ìš©, ì—†ìœ¼ë©´ ìˆœí™˜ ìƒ‰ìƒ
        colors[parser] = STYLE_PARSER_COLORS.get(parser, DEFAULT_COLORS[i % len(DEFAULT_COLORS)])
    return colors


# =============================================================================
# ë°ì´í„° ë¡œë“œ
# =============================================================================

@st.cache_data(ttl=300)
def load_data():
    """Load data with caching - scans results/test_*/ folders"""
    data = load_all_results_cached()
    if "error" in data:
        return data, True
    return data, False


# ë°ì´í„° ë¡œë“œ
raw_data, is_error = load_data()

# íŒŒì„œ ìƒ‰ìƒ
PARSER_NAMES = get_parser_names(raw_data)
PARSER_COLORS = get_parser_colors(PARSER_NAMES)

# ë³€í™˜ëœ ë°ì´í„° (í˜¸í™˜ì„± ìœ ì§€)
PARSING_DATA = get_parsing_data(raw_data)
CHUNKING_DATA = get_chunking_data(raw_data)

# ìƒˆë¡œìš´ í˜•ì‹ ë°ì´í„°
TEST_IDS = get_test_ids(raw_data)


# =============================================================================
# ì°¨íŠ¸ ìƒì„± í•¨ìˆ˜
# =============================================================================

def hex_to_rgba(hex_color: str, alpha: float = 0.1) -> str:
    """Hex ìƒ‰ìƒì„ rgbaë¡œ ë³€í™˜"""
    hex_color = hex_color.lstrip('#')
    r = int(hex_color[0:2], 16)
    g = int(hex_color[2:4], 16)
    b = int(hex_color[4:6], 16)
    return f"rgba({r},{g},{b},{alpha})"


def create_thin_bar_chart(data: Dict, metric: str, title: str,
                          lower_is_better: bool = False) -> go.Figure:
    """ì–‡ì€ ê°€ë¡œí˜• Bar Chart"""
    parsers = list(data["parsers"].keys())
    values = [data["parsers"][p].get(metric) or 0 for p in parsers]  # Handle None
    colors = [PARSER_COLORS.get(p, "#888") for p in parsers]

    fig = go.Figure()
    # Format based on metric type
    if metric == "elapsed_time":
        text_values = [f"{v:.1f}s" for v in values]
    else:
        text_values = [f"{v:.3f}" for v in values]

    fig.add_trace(go.Bar(
        y=parsers,
        x=values,
        orientation='h',
        marker_color=colors,
        marker_line_width=0,
        text=text_values,
        textposition="outside",
        textfont=dict(size=12, color="#333"),
    ))

    direction = "â† Lower is better" if lower_is_better else "Higher is better â†’"
    fig.update_layout(
        title=dict(text=title, font=dict(size=14, color="#1a1a2e"), x=0),
        height=180,
        margin=dict(l=10, r=80, t=40, b=25),
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)",
        font=dict(size=12, color="#666"),
        xaxis=dict(showgrid=False, showticklabels=False, zeroline=False),
        yaxis=dict(showgrid=False, tickfont=dict(size=12)),
        showlegend=False,
        annotations=[dict(
            text=direction, x=1, y=-0.12, xref="paper", yref="paper",
            showarrow=False, font=dict(size=10, color="#888"), xanchor="right"
        )]
    )
    return fig


def create_radar_chart(all_data: Dict) -> go.Figure:
    """íŒŒì„œë³„ ì„±ëŠ¥ Radar Chart"""
    metrics = ["WER", "CER", "Struct-F1", "Latency"]
    fig = go.Figure()

    for parser in PARSER_NAMES:
        values = []
        for metric_key in ["wer", "cer", "structure_f1", "elapsed_time"]:
            vals = [
                test["parsers"][parser].get(metric_key, 0)
                for test in all_data.values()
                if parser in test["parsers"]
            ]
            # Filter out None values
            vals = [v for v in vals if v is not None]
            avg = np.mean(vals) if vals else 0

            # ì •ê·œí™” (ë‚®ì„ìˆ˜ë¡ ì¢‹ì€ ê²ƒì€ ë°˜ì „, 0=worst, 1=best)
            if metric_key in ["wer", "cer"]:
                # WER/CER: 0% = 1.0 (best), 200%+ = 0.0 (worst)
                normalized = max(0, 1 - avg / 2)
            elif metric_key == "elapsed_time":
                # Latency: 0s = 1.0 (best), 120s+ = 0.0 (worst)
                normalized = max(0, 1 - avg / 120)
            elif metric_key == "structure_f1":
                # Structure F1: 0 = 0.0 (worst), 1 = 1.0 (best)
                normalized = avg
            else:
                normalized = avg
            values.append(normalized)

        values.append(values[0])  # ë‹«ê¸°

        fig.add_trace(go.Scatterpolar(
            r=values,
            theta=metrics + [metrics[0]],
            name=parser,
            line=dict(color=PARSER_COLORS.get(parser, "#888"), width=3),
            fill='toself',
            fillcolor=hex_to_rgba(PARSER_COLORS.get(parser, "#888"), 0.1),
        ))

    fig.update_layout(
        polar=dict(
            radialaxis=dict(visible=True, range=[0, 1], showticklabels=False, gridcolor="#E5E5E5"),
            angularaxis=dict(tickfont=dict(size=13, color="#333"), gridcolor="#E5E5E5"),
            bgcolor="rgba(0,0,0,0)",
        ),
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=-0.15, xanchor="center", x=0.5, font=dict(size=12)),
        height=450,
        margin=dict(l=80, r=80, t=40, b=80),
        paper_bgcolor="rgba(0,0,0,0)",
    )
    return fig


def create_bc_cs_scatter(chunking_data: Dict) -> go.Figure:
    """BC vs CS Scatter Plot"""
    fig = go.Figure()

    for strategy, data in chunking_data.items():
        bc_values = [c.get("bc", 0) for c in data.get("chunks", [])]
        cs_values = [c.get("cs", 0) for c in data.get("chunks", [])]

        if not bc_values:
            continue

        fig.add_trace(go.Scatter(
            x=bc_values, y=cs_values, mode='markers', name=strategy,
            marker=dict(
                size=12,
                color=STRATEGY_COLORS.get(strategy, "#888"),
                line=dict(width=1, color="white"),
                opacity=0.8,
            ),
            hovertemplate=f"<b>{strategy}</b><br>BC: %{{x:.2f}}<br>CS: %{{y:.2f}}<extra></extra>",
        ))

    # Quadrant ì˜ì—­
    fig.add_shape(type="rect", x0=0.5, x1=1, y0=0, y1=0.5,
                  fillcolor="rgba(16, 185, 129, 0.05)", line_width=0)
    fig.add_shape(type="rect", x0=0, x1=0.5, y0=0.5, y1=1,
                  fillcolor="rgba(239, 68, 68, 0.05)", line_width=0)

    fig.add_hline(y=0.5, line_dash="dot", line_color="#ccc", line_width=1)
    fig.add_vline(x=0.5, line_dash="dot", line_color="#ccc", line_width=1)

    annotations = [
        dict(x=0.75, y=0.25, text="ì´ìƒì <br>(BCâ†‘ CSâ†“)", showarrow=False,
             font=dict(size=9, color="#059669"), opacity=0.7),
        dict(x=0.25, y=0.75, text="Over-merge<br>(BCâ†“ CSâ†‘)", showarrow=False,
             font=dict(size=9, color="#DC2626"), opacity=0.7),
        dict(x=0.75, y=0.75, text="Fragmentation<br>(BCâ†‘ CSâ†‘)", showarrow=False,
             font=dict(size=9, color="#D97706"), opacity=0.7),
        dict(x=0.25, y=0.25, text="Structural<br>Failure", showarrow=False,
             font=dict(size=9, color="#6B7280"), opacity=0.7),
    ]

    fig.update_layout(
        title=dict(text="BCâ€“CS Distribution by Strategy", font=dict(size=14, color="#1a1a2e"), x=0),
        xaxis=dict(title="Boundary Clarity (BC) â†’", range=[0, 1], gridcolor="#E5E5E5", zeroline=False),
        yaxis=dict(title="Chunk Stickiness (CS) â†“", range=[0, 1], gridcolor="#E5E5E5", zeroline=False),
        height=450, margin=dict(l=60, r=30, t=50, b=50),
        paper_bgcolor="rgba(0,0,0,0)", plot_bgcolor="rgba(0,0,0,0)",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="left", x=0, font=dict(size=10)),
        annotations=annotations,
    )
    return fig


def create_grouped_bar(all_data: Dict, metric: str, title: str, lower_is_better: bool = False) -> go.Figure:
    """ì „ì²´ í…ŒìŠ¤íŠ¸ ë¹„êµ Grouped Bar Chart"""
    test_ids = [d["id"] for d in all_data.values()]
    fig = go.Figure()

    for parser in PARSER_NAMES:
        color = PARSER_COLORS.get(parser, "#888")
        values = [test["parsers"].get(parser, {}).get(metric) or 0 for test in all_data.values()]  # Handle None
        fig.add_trace(go.Bar(
            name=parser, x=test_ids, y=values,
            marker_color=color, marker_line_width=0,
            text=[f"{v:.2f}" if metric != "elapsed_time" else f"{v:.1f}s" for v in values],
            textposition="outside", textfont=dict(size=11), width=0.3,
        ))

    direction = "â†“ Lower is better" if lower_is_better else "â†‘ Higher is better"
    fig.update_layout(
        title=dict(text=f"{title} ({direction})", font=dict(size=15, color="#1a1a2e"), x=0),
        barmode="group", height=380,
        margin=dict(l=50, r=30, t=60, b=80),
        paper_bgcolor="rgba(0,0,0,0)", plot_bgcolor="rgba(0,0,0,0)",
        font=dict(size=12, color="#666"),
        xaxis=dict(showgrid=False, tickfont=dict(size=12)),
        yaxis=dict(gridcolor="#E5E5E5", gridwidth=0.5, zeroline=False, tickfont=dict(size=11)),
        legend=dict(orientation="h", yanchor="top", y=-0.12, xanchor="center", x=0.5, font=dict(size=11)),
        bargap=0.25, bargroupgap=0.1,
    )
    return fig


def create_metrics_comparison_subplot(all_data: Dict) -> go.Figure:
    """4ê°œ ë©”íŠ¸ë¦­ì„ í•˜ë‚˜ì˜ Subplotìœ¼ë¡œ í†µí•©í•œ ì°¨íŠ¸

    ì¥ì :
    - Legendê°€ í•œ ë²ˆë§Œ í‘œì‹œë¨ (ì¤‘ë³µ ì œê±°)
    - ì¼ê´€ëœ ë ˆì´ì•„ì›ƒ
    - í…ŒìŠ¤íŠ¸ ê°„ ë¹„êµê°€ ìš©ì´
    """
    test_ids = [d["id"] for d in all_data.values()]

    # ë©”íŠ¸ë¦­ ì •ì˜: (key, title, lower_is_better, format_func)
    metrics = [
        ("wer", "WER â†“", True, lambda v: f"{v:.2f}"),
        ("cer", "CER â†“", True, lambda v: f"{v:.2f}"),
        ("structure_f1", "Structure F1 â†‘", False, lambda v: f"{v:.2f}"),
        ("elapsed_time", "Latency â†“", True, lambda v: f"{v:.1f}s"),
    ]

    # 2x2 ì„œë¸Œí”Œë¡¯ ìƒì„± (ìƒí•˜ ê°„ê²© ë„“ê²Œ)
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=[m[1] for m in metrics],
        horizontal_spacing=0.10,
        vertical_spacing=0.18,
    )

    # ê° ë©”íŠ¸ë¦­ë³„ë¡œ ë°” ì¶”ê°€
    for idx, (metric_key, title, lower_is_better, fmt) in enumerate(metrics):
        row = idx // 2 + 1
        col = idx % 2 + 1

        for parser_idx, parser in enumerate(PARSER_NAMES):
            color = PARSER_COLORS.get(parser, "#888")
            values = [
                test["parsers"].get(parser, {}).get(metric_key) or 0
                for test in all_data.values()
            ]

            # ì²« ë²ˆì§¸ ì„œë¸Œí”Œë¡¯ì—ì„œë§Œ legend í‘œì‹œ
            show_legend = (idx == 0)

            fig.add_trace(
                go.Bar(
                    name=parser,
                    x=test_ids,
                    y=values,
                    marker_color=color,
                    marker_line_width=0,
                    text=[fmt(v) for v in values],
                    textposition="outside",
                    textfont=dict(size=10),
                    showlegend=show_legend,
                    legendgroup=parser,  # legend ê·¸ë£¹í•‘
                ),
                row=row, col=col
            )

    # ë ˆì´ì•„ì›ƒ ì„¤ì •
    fig.update_layout(
        height=650,
        barmode="group",
        paper_bgcolor="rgba(0,0,0,0)",
        plot_bgcolor="rgba(0,0,0,0)",
        font=dict(size=11, color="#666"),
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.04,
            xanchor="left",
            x=0,
            font=dict(size=14),
        ),
        margin=dict(l=50, r=30, t=100, b=40),
        bargap=0.15,
        bargroupgap=0.05,
    )

    # ê° ì¶• ì„¤ì •
    for i in range(1, 5):
        fig.update_xaxes(showgrid=False, tickfont=dict(size=10), row=(i-1)//2+1, col=(i-1)%2+1)
        fig.update_yaxes(gridcolor="#E5E5E5", gridwidth=0.5, zeroline=False, tickfont=dict(size=10), row=(i-1)//2+1, col=(i-1)%2+1)

    # ì„œë¸Œí”Œë¡¯ íƒ€ì´í‹€ ìŠ¤íƒ€ì¼ (í¬ê²Œ, ì¢Œì¸¡ ì •ë ¬)
    for annotation in fig['layout']['annotations']:
        annotation['font'] = dict(size=15, color="#1a1a2e", weight="bold")
        annotation['xanchor'] = 'left'
        # ì¢Œì¸¡ ì •ë ¬ì„ ìœ„í•´ x ìœ„ì¹˜ ì¡°ì • (ê° ì„œë¸Œí”Œë¡¯ì˜ ì‹œì‘ì )
        if annotation['x'] < 0.5:
            annotation['x'] = 0.0
        else:
            annotation['x'] = 0.55

    return fig


# =============================================================================
# ë©”ì¸ ëŒ€ì‹œë³´ë“œ
# =============================================================================

# í—¤ë”
st.title("ğŸ“„ VLM Document Parsing Quality Analysis")
st.caption(f"CLI í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì‹œê°í™” | Tech Report ì‘ì„± ì§€ì› | {VERSION}")

# ì—ëŸ¬ ê²½ê³ 
if is_error:
    st.error(f"âš ï¸ {raw_data.get('error', 'í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')}")
    st.info("í…ŒìŠ¤íŠ¸ ì‹¤í–‰: `python -m src.eval_parsers --all`")
    st.stop()

# ë°ì´í„° ì •ë³´
data_info_cols = st.columns([1, 1, 1, 2])
with data_info_cols[0]:
    st.metric("Total Tests", raw_data.get("test_count", len(PARSING_DATA)))
with data_info_cols[1]:
    st.metric("Parsers", len(PARSER_NAMES))
with data_info_cols[2]:
    chunking_tests = len(get_tests_with_chunking(raw_data))
    st.metric("Chunking Tests", chunking_tests)
with data_info_cols[3]:
    loaded_at = raw_data.get("loaded_at", "N/A")
    st.caption(f"Data Version: {raw_data.get('version', 'N/A')} | Loaded: {loaded_at}")

st.markdown("---")

# íƒ­ êµ¬ì„±
tab_parsing, tab_chunking, tab_result = st.tabs([
    "ğŸ” Parsing Test",
    "ğŸ“¦ Chunking Test",
    "ğŸ“Š ì¢…í•© ë¶„ì„"
])


# =============================================================================
# TAB 1: Parsing Test
# =============================================================================

with tab_parsing:
    st.markdown("## Parsing Test Results")

    # Metrics ì •ì˜
    with st.expander("ğŸ“ Metrics ì •ì˜", expanded=False):
        col1, col2 = st.columns(2)
        with col1:
            st.markdown("**WER (Word Error Rate)** Â· :green[â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ]")
            st.markdown("ë‹¨ì–´ ë‹¨ìœ„ ì˜¤ë¥˜ìœ¨. ì‚½ì…/ì‚­ì œ/ëŒ€ì²´ ì˜¤ë¥˜ ì¢…í•©.")
            st.markdown("**CER (Character Error Rate)** Â· :green[â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ]")
            st.markdown("ë¬¸ì ë‹¨ìœ„ ì˜¤ë¥˜ìœ¨. ëˆ„ë½/ì¶”ê°€/ë³€ê²½ ë¬¸ì ì¶”ì .")
        with col2:
            st.markdown("**Structure F1** Â· :orange[â†‘ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ]")
            st.markdown("ë§ˆí¬ë‹¤ìš´ êµ¬ì¡° ìš”ì†Œ(í—¤ë”©, ë¦¬ìŠ¤íŠ¸, í…Œì´ë¸”) ê²€ì¶œ F1 ìŠ¤ì½”ì–´.")
            st.markdown("**Latency** Â· :green[â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ]")
            st.markdown("ë¬¸ì„œ 1ê±´ Parsing ì²˜ë¦¬ ì‹œê°„ (ì´ˆ).")

    st.markdown("---")

    # Global Performance Summary
    st.markdown("### ğŸ“ˆ Global Performance Summary")

    col_table, col_radar = st.columns([1, 1])

    with col_table:
        # DataFrame ìƒì„±
        summary_df = get_parsing_summary_df(raw_data)
        # Use available columns from new format (including Structure F1)
        display_df = summary_df[["Test ID", "Parser", "CER %", "WER %", "Struct-F1 %", "Latency (s)", "Success"]].copy()
        display_df = display_df.rename(columns={
            "Test ID": "Test",
            "Struct-F1 %": "Struct-F1",
            "Latency (s)": "Latency",
        })
        display_df["Latency"] = display_df["Latency"].apply(lambda x: f"{x:.1f}s")

        st.dataframe(display_df, use_container_width=True, hide_index=True, height=350)

        # CSV ë‹¤ìš´ë¡œë“œ
        csv_data = export_df_to_csv(summary_df)
        st.download_button(
            label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
            data=csv_data,
            file_name="parsing_summary.csv",
            mime="text/csv",
        )

    with col_radar:
        radar_fig = create_radar_chart(PARSING_DATA)
        st.plotly_chart(
            radar_fig,
            use_container_width=True,
            config=get_chart_download_config("radar_chart")
        )

    # Metrics Comparison - í†µí•© Subplot ì°¨íŠ¸
    st.markdown("#### Metrics Comparison")

    metrics_fig = create_metrics_comparison_subplot(PARSING_DATA)
    st.plotly_chart(
        metrics_fig,
        use_container_width=True,
        config=get_chart_download_config("metrics_comparison")
    )

    st.markdown("---")

    # Detailed Test Analysis with Pagination
    st.markdown("### ğŸ”¬ Detailed Test Analysis")

    # í˜ì´ì§€ë„¤ì´ì…˜ (10ê°œ ì´ˆê³¼ ì‹œ)
    test_items = list(PARSING_DATA.items())
    total_tests = len(test_items)

    if total_tests > PAGE_SIZE:
        # í˜ì´ì§€ ì„ íƒ
        col_page_info, col_page_nav = st.columns([2, 1])

        with col_page_info:
            st.caption(f"ì´ {total_tests}ê°œ í…ŒìŠ¤íŠ¸ (í˜ì´ì§€ë‹¹ {PAGE_SIZE}ê°œ)")

        # í˜ì´ì§€ ìƒíƒœ
        if "parsing_page" not in st.session_state:
            st.session_state.parsing_page = 1

        total_pages = (total_tests + PAGE_SIZE - 1) // PAGE_SIZE

        with col_page_nav:
            page = st.number_input(
                "Page",
                min_value=1,
                max_value=total_pages,
                value=st.session_state.parsing_page,
                key="parsing_page_input"
            )
            st.session_state.parsing_page = page

        # í˜„ì¬ í˜ì´ì§€ ë°ì´í„°
        paginated_items, _, _, _ = paginate_data(test_items, page, PAGE_SIZE)
    else:
        paginated_items = test_items

    # í…ŒìŠ¤íŠ¸ë³„ ìƒì„¸ (Lazy Loading via Expander)
    for test_id, test_data in paginated_items:
        # ë©”íƒ€ë°ì´í„°ì—ì„œ ì •ë³´ ì¶”ì¶œ (ìë™ ì¶”ì¶œ í˜•ì‹)
        metadata = test_data.get("metadata", {})
        title = metadata.get("title", test_data.get('id', test_id))
        filename = metadata.get("filename", test_data.get('name', ''))
        doc_type = metadata.get("doc_type", test_data.get('doc_type', 'unknown'))
        pages = metadata.get("pages", test_data.get('pages', 0))
        file_size_kb = metadata.get("file_size_kb", test_data.get('file_size_kb', 0))
        language = metadata.get("language", test_data.get('language', ''))
        has_text_layer = metadata.get("has_text_layer", test_data.get('has_text_layer', False))

        # test_idì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ (ì˜ˆ: test_1 â†’ 1)
        test_num = test_id.replace("test_", "").replace("_", " ").title()

        # Expander ì œëª©: "ğŸ“„ Test 1: filename.pdf (PDF, 5p)"
        page_info = f", {pages}p" if pages else ""
        expander_title = f"ğŸ“„ **Test {test_num}**: {filename} ({doc_type}{page_info})"

        with st.expander(expander_title, expanded=False):
            # íŒŒì¼ ì •ë³´ í‘œì‹œ
            info_cols = st.columns([2, 1, 1, 1])
            with info_cols[0]:
                st.caption(f"ğŸ“ {title}")
            with info_cols[1]:
                if file_size_kb:
                    size_str = f"{file_size_kb:.0f}KB" if file_size_kb < 1024 else f"{file_size_kb/1024:.1f}MB"
                    st.caption(f"ğŸ’¾ {size_str}")
            with info_cols[2]:
                if language:
                    st.caption(f"ğŸŒ {language.upper()}")
            with info_cols[3]:
                text_layer_icon = "âœ…" if has_text_layer else "âŒ"
                st.caption(f"ğŸ“ Text: {text_layer_icon}")
            st.divider()

            # í…Œì´ë¸”
            detail_rows = []
            for parser, metrics in test_data["parsers"].items():
                structure_f1 = metrics.get('structure_f1')
                struct_f1_display = f"{structure_f1:.3f}" if structure_f1 is not None else "N/A"

                detail_rows.append({
                    "Parser": parser,
                    "WER â†“": f"{metrics.get('wer') or 0:.3f}",
                    "CER â†“": f"{metrics.get('cer') or 0:.3f}",
                    "Struct-F1 â†‘": struct_f1_display,
                    "Latency â†“": f"{metrics.get('elapsed_time') or 0:.1f}s",
                })
            st.dataframe(pd.DataFrame(detail_rows), use_container_width=True, hide_index=True)

            # Bar Charts
            chart_cols = st.columns(2)
            with chart_cols[0]:
                st.plotly_chart(
                    create_thin_bar_chart(test_data, "wer", "WER", lower_is_better=True),
                    use_container_width=True,
                    config=get_chart_download_config(f"{test_id}_wer")
                )
                st.plotly_chart(
                    create_thin_bar_chart(test_data, "cer", "CER", lower_is_better=True),
                    use_container_width=True,
                    config=get_chart_download_config(f"{test_id}_cer")
                )
            with chart_cols[1]:
                st.plotly_chart(
                    create_thin_bar_chart(test_data, "structure_f1", "Structure F1", lower_is_better=False),
                    use_container_width=True,
                    config=get_chart_download_config(f"{test_id}_structure_f1")
                )
                st.plotly_chart(
                    create_thin_bar_chart(test_data, "elapsed_time", "Latency", lower_is_better=True),
                    use_container_width=True,
                    config=get_chart_download_config(f"{test_id}_latency")
                )


# =============================================================================
# TAB 2: Chunking Test
# =============================================================================

with tab_chunking:
    st.markdown("## Semantic Chunking í’ˆì§ˆ ë¶„ì„")
    st.markdown("> **ì—°êµ¬ ê°€ì„¤**: êµ¬ì¡°í™”ëœ íŒŒì‹± ê²°ê³¼ê°€ ìœ ì‚¬ë„ ê¸°ë°˜ ì²­í‚¹ í’ˆì§ˆì— ê¸ì •ì  ì˜í–¥ì„ ë¯¸ì¹œë‹¤")

    # =========================================================================
    # ë°ì´í„° ìƒíƒœ í™•ì¸ ë° ìš”ì•½ ëŒ€ì‹œë³´ë“œ
    # =========================================================================

    # ì²­í‚¹ ê²°ê³¼ê°€ ìˆëŠ” í…ŒìŠ¤íŠ¸ í™•ì¸
    tests_with_chunking = get_tests_with_chunking(raw_data)
    chunking_parsers = get_chunking_parsers(raw_data)

    # ë°ì´í„° ìƒíƒœ íŒë‹¨
    def get_chunking_status(test_data: dict) -> dict:
        """ì²­í‚¹ ë°ì´í„° ìƒíƒœ ë¶„ì„"""
        chunking = test_data.get("chunking", {})
        config = chunking.get("config", {})
        results = chunking.get("results", {})

        status = {
            "has_chunks": bool(results),
            "has_bc": False,
            "has_cs": False,
            "is_mock": config.get("use_mock", False),
            "strategy": config.get("strategy", "unknown"),
            "llm_model": config.get("llm_model", "N/A"),
            "chunk_counts": {},
        }

        for parser, result in results.items():
            status["chunk_counts"][parser] = result.get("chunk_count", 0)
            if result.get("bc", {}).get("score") is not None:
                status["has_bc"] = True
            if result.get("cs", {}).get("score") is not None:
                status["has_cs"] = True

        return status

    # ì „ì²´ ìƒíƒœ ì§‘ê³„
    all_statuses = {}
    for test_id in tests_with_chunking:
        test_data = raw_data.get("tests", {}).get(test_id, {})
        all_statuses[test_id] = get_chunking_status(test_data)

    # =========================================================================
    # ìƒíƒœ í‘œì‹œ ë°°ë„ˆ
    # =========================================================================
    if not tests_with_chunking:
        st.warning("ì²­í‚¹ í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        st.markdown("### ì‹¤í—˜ ì‹¤í–‰ ê°€ì´ë“œ")
        st.markdown("""
        ```bash
        # 1. ë¨¼ì € íŒŒì‹± í…ŒìŠ¤íŠ¸ ì‹¤í–‰
        uv run python src/test_parsers.py --test-id test_5

        # 2. ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (Mock - ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)
        uv run python src/eval_chunking.py --test-id test_5 --use-mock

        # 3. ì²­í‚¹ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (Real BC - OpenAI API ì‚¬ìš©)
        uv run python src/eval_chunking.py --test-id test_5 --skip-cs
        ```
        """)
    else:
        # ìƒíƒœ ìš”ì•½
        has_any_bc = any(s["has_bc"] for s in all_statuses.values())
        has_any_cs = any(s["has_cs"] for s in all_statuses.values())
        any_mock = any(s["is_mock"] for s in all_statuses.values())

        # ìƒíƒœ ë°°ì§€
        col_status1, col_status2, col_status3, col_status4 = st.columns(4)

        with col_status1:
            st.metric(
                "í…ŒìŠ¤íŠ¸ ìˆ˜",
                len(tests_with_chunking),
                help="ì²­í‚¹ ê²°ê³¼ê°€ ìˆëŠ” í…ŒìŠ¤íŠ¸ ìˆ˜"
            )

        with col_status2:
            if has_any_bc:
                if any_mock:
                    st.metric("BC ë°ì´í„°", "Mock", delta="âš ï¸ ì¶”ì •ì¹˜", delta_color="off")
                else:
                    st.metric("BC ë°ì´í„°", "Real", delta="âœ“ LLM ê¸°ë°˜", delta_color="off")
            else:
                st.metric("BC ë°ì´í„°", "ì—†ìŒ", delta="ì²­í¬ë§Œ ìƒì„±ë¨", delta_color="off")

        with col_status3:
            if has_any_cs:
                st.metric("CS ë°ì´í„°", "ìˆìŒ", delta="âœ“ ê³„ì‚°ë¨", delta_color="off")
            else:
                st.metric("CS ë°ì´í„°", "ì—†ìŒ", delta="--skip-cs ì‚¬ìš©ë¨", delta_color="off")

        with col_status4:
            total_chunks = sum(
                sum(s["chunk_counts"].values())
                for s in all_statuses.values()
            )
            st.metric("ì´ ì²­í¬ ìˆ˜", f"{total_chunks:,}")

        # Mock ë°ì´í„° ê²½ê³ 
        if any_mock and has_any_bc:
            st.info(
                "**Mock ë°ì´í„° ì‚¬ìš© ì¤‘**: BC/CS ê°’ì€ ë‹¨ì–´ í†µê³„ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹± ì¶”ì •ì¹˜ì…ë‹ˆë‹¤. "
                "ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ `--skip-mock` ì˜µì…˜ìœ¼ë¡œ OpenAI APIë¥¼ ì‚¬ìš©í•˜ì„¸ìš”."
            )

        st.markdown("---")

        # =========================================================================
        # í…ŒìŠ¤íŠ¸ë³„ ì²­í‚¹ ìš”ì•½
        # =========================================================================
        st.markdown("### í…ŒìŠ¤íŠ¸ë³„ ì²­í‚¹ ê²°ê³¼ ìš”ì•½")

        for test_id in tests_with_chunking:
            status = all_statuses[test_id]

            # ìƒíƒœ ì•„ì´ì½˜
            if status["has_bc"] and not status["is_mock"]:
                status_icon = "âœ…"
                status_text = "Real BC"
            elif status["has_bc"] and status["is_mock"]:
                status_icon = "âš ï¸"
                status_text = "Mock BC"
            else:
                status_icon = "ğŸ“¦"
                status_text = "Chunks Only"

            with st.expander(
                f"{status_icon} **{test_id}** | {status_text} | Strategy: {status['strategy']} | "
                f"Chunks: {sum(status['chunk_counts'].values())}"
            ):
                # ì²­í¬ ìˆ˜ ë¹„êµ í…Œì´ë¸”
                chunk_df_data = []
                for parser, count in status["chunk_counts"].items():
                    chunk_df_data.append({
                        "Parser": parser,
                        "Chunk Count": count,
                        "Strategy": status["strategy"],
                        "LLM Model": status["llm_model"],
                    })

                if chunk_df_data:
                    chunk_df = pd.DataFrame(chunk_df_data)
                    st.dataframe(chunk_df, use_container_width=True, hide_index=True)

                # BC/CS ë°ì´í„°ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                if status["has_bc"]:
                    test_data = raw_data.get("tests", {}).get(test_id, {})
                    chunking_results = test_data.get("chunking", {}).get("results", {})

                    bc_cs_data = []
                    for parser, result in chunking_results.items():
                        bc = result.get("bc", {})
                        cs = result.get("cs", {})
                        bc_cs_data.append({
                            "Parser": parser,
                            "BC (â†‘)": f"{bc.get('score', 0):.4f}" if bc.get('score') is not None else "N/A",
                            "BC Std": f"Â±{bc.get('std', 0):.4f}" if bc.get('std') is not None else "-",
                            "CS (â†“)": f"{cs.get('score', 0):.4f}" if cs.get('score') is not None else "N/A",
                            "CS Std": f"Â±{cs.get('std', 0):.4f}" if cs.get('std') is not None else "-",
                        })

                    if bc_cs_data:
                        st.markdown("**BC / CS Metrics**")
                        bc_cs_df = pd.DataFrame(bc_cs_data)
                        st.dataframe(bc_cs_df, use_container_width=True, hide_index=True)

        st.markdown("---")

        # =========================================================================
        # Parser ë¹„êµ ì°¨íŠ¸ (BC/CS ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ)
        # =========================================================================
        if has_any_bc:
            st.markdown("### Parserë³„ BC/CS ë¹„êµ")

            # ë¹„êµ ê°€ëŠ¥í•œ ì „ëµ ìˆ˜ì§‘
            all_strategies = set()
            for parser in chunking_parsers:
                strategies = get_chunking_data_for_parser(raw_data, parser)
                for s in strategies:
                    all_strategies.add(s.get("strategy", "unknown"))

            strategy_list = sorted(list(all_strategies))

            if strategy_list:
                selected_strategy = st.selectbox(
                    "ë¹„êµí•  ì „ëµ",
                    options=strategy_list,
                    index=strategy_list.index("semantic") if "semantic" in strategy_list else 0,
                    key="chunking_strategy_select"
                )

                # ë¹„êµ ì°¨íŠ¸
                comparison_fig = create_parser_chunking_comparison(
                    CHUNKING_DATA,
                    selected_strategy,
                    title=f"Parserë³„ Chunking í’ˆì§ˆ ë¹„êµ"
                )
                st.plotly_chart(
                    comparison_fig,
                    use_container_width=True,
                    config=get_chart_download_config(f"parser_comparison_{selected_strategy}")
                )

            st.markdown("---")

        # =========================================================================
        # Parserë³„ ìƒì„¸ ë¶„ì„
        # =========================================================================
        if chunking_parsers and chunking_parsers != ["_legacy"]:
            st.markdown("### Parserë³„ ìƒì„¸ ë¶„ì„")

            selected_parser = st.selectbox(
                "ë¶„ì„í•  Parser ì„ íƒ",
                options=chunking_parsers,
                key="chunking_parser_select"
            )

            parser_strategies = get_chunking_data_for_parser(raw_data, selected_parser)

            if parser_strategies:
                # BC/CS ì°¨íŠ¸ (2ì—´)
                chart_col1, chart_col2 = st.columns(2)

                with chart_col1:
                    st.markdown("##### Boundary Clarity Flow")
                    st.caption("ë¬¸ì¥ë³„ BC ê°’ (ë†’ì„ìˆ˜ë¡ ê²½ê³„ í’ˆì§ˆ ìš°ìˆ˜)")

                    bc_flow_fig = create_bc_document_flow(
                        parser_strategies,
                        title=f"{selected_parser}: BC by Sentence"
                    )
                    st.plotly_chart(
                        bc_flow_fig,
                        use_container_width=True,
                        config=get_chart_download_config(f"bc_flow_{selected_parser}")
                    )

                with chart_col2:
                    st.markdown("##### Chunk Stickiness")
                    st.caption("ì²­í¬ ë‚´ë¶€ ìœ ì‚¬ë„ (ë‚®ì„ìˆ˜ë¡ ë…ë¦½ì )")

                    cs_bar_fig = create_cs_mean_std_bar(
                        parser_strategies,
                        title=f"{selected_parser}: Intra-chunk Similarity"
                    )
                    st.plotly_chart(
                        cs_bar_fig,
                        use_container_width=True,
                        config=get_chart_download_config(f"cs_bar_{selected_parser}")
                    )
            else:
                st.info(f"{selected_parser}ì˜ BC/CS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    st.markdown("---")

    # =========================================================================
    # BC / CS ê°œë… ì„¤ëª… (ì ‘ê¸°)
    # =========================================================================
    with st.expander("ğŸ“ BC / CS Metrics ì´í•´í•˜ê¸°", expanded=False):
        col_bc, col_cs = st.columns(2)

        with col_bc:
            st.markdown("""
            ### Boundary Clarity (BC)
            **ì¸ì ‘ ì²­í¬ ê°„ ì˜ë¯¸ì  ë…ë¦½ì„±** Â· :green[â†‘ ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ]

            ```
            BC = 1 - cosine_similarity(chunk_i, chunk_i+1)

            ì¸ì ‘ ì²­í¬ ê°„ ì„ë² ë”© ê±°ë¦¬ (Semantic Distance)
            ```

            - **BC > 0.5**: ìš°ìˆ˜í•œ ê²½ê³„ í’ˆì§ˆ
            - **BC â‰ˆ 1.0**: ì´ìƒì  (ì²­í¬ê°€ ì˜ë¯¸ì ìœ¼ë¡œ ë…ë¦½)
            - **BC < 0.3**: ì²­í¬ ê²½ê³„ ì¬ê²€í†  í•„ìš”
            """)

        with col_cs:
            st.markdown("""
            ### Chunk Stickiness (CS)
            **ì²­í¬ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  ì—”íŠ¸ë¡œí”¼** Â· :orange[â†“ ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ]

            ```
            CS = -Î£ (h_i / 2m) Ã— logâ‚‚(h_i / 2m)

            h_i: ë…¸ë“œ iì˜ ê°€ì¤‘ ì°¨ìˆ˜
            m: ì „ì²´ ì—£ì§€ ê°€ì¤‘ì¹˜ í•© / 2
            ```

            - **CS < 1.0**: ì²­í¬ê°€ ë…ë¦½ì  (ì¢‹ìŒ)
            - **CS > 2.0**: ê³¼ë„í•œ ì—°ê²°ì„± (ë¶„í•  í•„ìš”)
            - ì„ë² ë”© ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë˜í”„ êµ¬ì¶•
            """)

        st.markdown("---")
        st.markdown("""
        | ì§€í‘œ | ì˜ë¯¸ | ì´ìƒì  ê°’ | í•´ì„ |
        |------|------|----------|------|
        | **BC â†‘** | Boundary Clarity | > 0.5 | ì¸ì ‘ ì²­í¬ê°€ ì˜ë¯¸ì ìœ¼ë¡œ ë…ë¦½ì  |
        | **CS â†“** | Chunk Stickiness | < 1.0 | ì²­í¬ ê·¸ë˜í”„ê°€ í¬ì†Œí•¨ (ë…ë¦½ì ) |
        | **std_bc â†“** | BC í‘œì¤€í¸ì°¨ | < 0.15 | ì¼ê´€ëœ ê²½ê³„ í’ˆì§ˆ |

        *BCëŠ” Semantic Distance ê¸°ë°˜ (1 - cosine similarity), CSëŠ” Structural Entropy ê¸°ë°˜*
        """)

    # =========================================================================
    # ì‹¤í—˜ ì‹¤í–‰ ê°€ì´ë“œ (ì ‘ê¸°)
    # =========================================================================
    with st.expander("ğŸš€ ì‹¤í—˜ ì‹¤í–‰ ê°€ì´ë“œ", expanded=False):
        st.markdown("""
        ### ì²­í‚¹ í’ˆì§ˆ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ë²•

        **1. Mock ëª¨ë“œ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸)**
        ```bash
        uv run python src/eval_chunking.py --test-id test_5 --use-mock
        ```
        - ë‹¨ì–´ í†µê³„ ê¸°ë°˜ íœ´ë¦¬ìŠ¤í‹±
        - API í˜¸ì¶œ ì—†ìŒ, ì¦‰ì‹œ ì™„ë£Œ

        **2. Real BC ëª¨ë“œ (OpenAI API)**
        ```bash
        # .envì— OPENAI_API_KEY ì„¤ì • í•„ìš”
        uv run python src/eval_chunking.py --test-id test_5 --skip-cs
        ```
        - GPT-4o ê¸°ë°˜ Perplexity ê³„ì‚°
        - BC: ~2Ã—(N-1) API í˜¸ì¶œ
        - CS ìŠ¤í‚µìœ¼ë¡œ ì‹œê°„ ì ˆì•½

        **3. Full ëª¨ë“œ (BC + CS)**
        ```bash
        uv run python src/eval_chunking.py --test-id test_5
        ```
        - BC + CS ëª¨ë‘ ê³„ì‚°
        - CS: NÂ² API í˜¸ì¶œ (ëŒ€ê·œëª¨ ë¬¸ì„œëŠ” ë¹„ìš© ì£¼ì˜)

        ### ì˜ˆìƒ ì†Œìš” ì‹œê°„ (N=350 ì²­í¬ ê¸°ì¤€)
        | ëª¨ë“œ | API í˜¸ì¶œ | ì˜ˆìƒ ì‹œê°„ |
        |------|---------|----------|
        | Mock | 0 | < 1ë¶„ |
        | BC Only | ~700 | ~10ë¶„ |
        | BC + CS | ~122,500 | ìˆ˜ ì‹œê°„ |
        """)


# =============================================================================
# TAB 3: ì¢…í•© ë¶„ì„
# =============================================================================

with tab_result:
    st.markdown("## ğŸ“Š ì¢…í•© ë¶„ì„ ê²°ê³¼")
    st.markdown("> Parsingê³¼ Chunking ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ íŒŒì´í”„ë¼ì¸ í’ˆì§ˆì„ ì§„ë‹¨í•©ë‹ˆë‹¤.")

    st.markdown("---")

    st.markdown("### ğŸ¯ í•µì‹¬ ë°œê²¬ì‚¬í•­")

    col1, col2 = st.columns(2)

    with col1:
        st.markdown("""
        #### Parsing ê´€ì 

        1. **VLMì´ ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜**
           - ëŒ€ë¶€ë¶„ì˜ í…ŒìŠ¤íŠ¸ì—ì„œ ìµœì € WER ë‹¬ì„±
           - íŠ¹íˆ ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì„œì—ì„œ ì••ë„ì 

        2. **Trade-off ì¡´ì¬**
           - ì •í™•ë„ â†” ì²˜ë¦¬ ì‹œê°„
           - ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ì—ëŠ” pdfplumber ê³ ë ¤

        3. **ë¬¸ì„œ ìœ í˜•ë³„ ì°¨ì´ í¼**
           - ìŠ¤ìº” ì´ë¯¸ì§€: VLM í•„ìˆ˜
           - ë””ì§€í„¸ PDF: pdfplumberë„ ì¶©ë¶„
        """)

    with col2:
        st.markdown("""
        #### Chunking ê´€ì 

        1. **Semantic Chunking ê¶Œì¥**
           - BCê°€ ê°€ì¥ ë†’ì€ ê²½ê³„ ëª…í™•ë„
           - CSê°€ ë‚®ì€ ë‚´ë¶€ ì˜ì¡´ì„±

        2. **Fixed Chunking ì£¼ì˜**
           - ì˜ë¯¸ ê²½ê³„ ë¬´ì‹œë¡œ BC ë‚®ìŒ
           - RAG ì„±ëŠ¥ ì €í•˜ ìš°ë ¤

        3. **ìµœì  íŒŒë¼ë¯¸í„°**
           - Chunk Size: 400-600
           - Overlap: 50-100
        """)

    st.markdown("---")

    st.markdown("### ğŸš€ ë‹¤ìŒ ë‹¨ê³„")
    st.markdown("""
    | ìš°ì„ ìˆœìœ„ | ì‘ì—… | ëª©ì  |
    |---------|------|------|
    | 1 | Golden Dataset êµ¬ì¶• | í‰ê°€ ì‹ ë¢°ë„ í–¥ìƒ |
    | 2 | VLM SFT í•™ìŠµ | êµ¬ì¡°í™” ì„±ëŠ¥ ê°œì„  |
    | 3 | Semantic Chunking ì ìš© | RAG í’ˆì§ˆ í–¥ìƒ |
    | 4 | ì¶”ê°€ ë¬¸ì„œ ìœ í˜• í…ŒìŠ¤íŠ¸ | ì¼ë°˜í™” ê²€ì¦ |
    """)

    st.markdown("---")
    st.caption(f"VLM Document Parsing Quality Analysis | {VERSION}")
