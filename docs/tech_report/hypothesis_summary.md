# Hypothesis Summary: CER Evaluation Fairness

> **Date**: 2026-01-30
> **Context**: 국제 학회 제출 논문 — 실험 가설 및 검증 결과 정리

---

## 1. 연구 배경

PDF 문서 파싱 품질을 평가하기 위해 **Character Error Rate (CER)** 를 사용한다.
Ground Truth(GT)는 LaTeX 소스에서 pandoc으로 변환한 Markdown이고,
추출 텍스트는 PyMuPDF로 PDF에서 추출한 raw text이다.

초기 실험 결과 **평균 CER ≈ 40%** 가 관측되었으며,
이 수치가 실제 추출 오류를 반영하는지, 아니면 평가 방법론의 아티팩트인지를 규명해야 했다.

---

## 2. 귀무가설 (H₀)

> **CER 40%는 GT와 추출 텍스트 간의 실제 콘텐츠·구조적 차이를 반영하며,
> `normalize_text()` 양측 동일 적용 방식의 정규화로는 유의미하게 개선할 수 없다.**

### H₀의 전제

1. pandoc GT와 PyMuPDF 추출은 **동일 LaTeX 소스에서 서로 다른 경로**를 거치므로,
   콘텐츠 구성이 구조적으로 다르다 (참고문헌 유무, 인용 형식, 수식 표현, 페이지 번호 등).
2. `normalize_text()`는 GT와 추출 **양쪽에 동일하게 적용**되므로,
   한쪽에만 존재하는 콘텐츠를 제거하면 해당 쪽만 짧아져 CER이 악화된다.
3. CER 40%의 구성 (Substitution 17.4%, Insertion 19%, Deletion 4.1%)은
   읽기 순서 차이(2-column), 추출 전용 콘텐츠(References, 페이지 번호), GT 전용 아티팩트(pandoc div, 수식)의 합산이다.

---

## 3. 대립가설 (H₁) — PRD v1.0

> **CER 40%의 상당 부분은 측정 노이즈(페이지 번호, 러닝 헤더, 참고문헌, 이메일 등)이며,
> `normalize_text()`에서 이를 제거하면 CER을 20% 이하로 개선할 수 있다.**

### H₁의 예상 효과 (PRD v1.0 기준)

| 정규화 항목 | 예상 CER 개선 |
|------------|--------------|
| 참고문헌 섹션 제거 (FR-001) | 5~15pp |
| 페이지 번호 제거 (FR-002) | 2~5pp |
| 러닝 헤더 제거 (FR-003) | 1~3pp |
| 이메일/arXiv ID 제거 (FR-004) | 1~2pp |
| **합계 예상** | **9~25pp → CER < 20%** |

---

## 4. 실험 검증 결과

### 4.1 각 정규화 항목의 실측 CER 변화

| 정규화 항목 | 예상 | 실측 CER | 변화 | 판정 |
|------------|------|---------|------|------|
| 참고문헌 제거 | -5~15pp | 40% → **56%** | **+16pp 악화** | H₁ 기각 |
| 이메일 줄 제거 | -1~2pp | 40% → **48%** | **+8pp 악화** | H₁ 기각 |
| 페이지 번호 제거 | -2~5pp | 40% → 39.6% | -0.4pp (무의미) | H₁ 기각 |
| 러닝 헤더 감지 | -1~3pp | 표 셀 대량 오탐 | 적용 불가 | H₁ 기각 |
| 숫자 인용 `[1]` 제거 | -2~3pp | 40% → **58%** | **+18pp 악화** | H₁ 기각 |
| 수평선 `---` 제거 | -0.5pp | 악화 | 악화 | H₁ 기각 |
| 괄호 인용 제거 | -0.5pp | 악화 | 악화 | H₁ 기각 |
| 각주 정의 제거 | ~0pp | 40% → 40.4% | +0.4pp (무시) | 중립 |

### 4.2 Body CER (참고문헌 분리 후 본문만 비교)

| 메트릭 | 값 | 비고 |
|--------|-----|------|
| Full CER | 40.4% | 전체 텍스트 비교 |
| Body CER | **63.1%** | 본문만 비교 (References 제거 후) |

Body CER이 Full CER보다 **23pp 더 높다**. 이는 추출 텍스트의 References 부분이
edit distance 정렬에서 GT 콘텐츠와 부분 매칭되어 전체 CER을 낮추는 역할을 하고 있었음을 의미한다.

### 4.3 악화 메커니즘

```
normalize_text() 양측 동일 적용
    │
    ├── GT에 없는 콘텐츠 제거 (References, 이메일, 페이지번호)
    │   → 추출만 짧아짐 → Deletion 폭증 → CER 악화
    │
    ├── 추출에 없는 콘텐츠 제거 (pandoc div, 수식)
    │   → GT만 짧아짐 → Insertion 폭증 → CER 악화
    │
    └── 양쪽 모두 존재하는 콘텐츠 제거 ([@key] 인용, [^N] 각주)
        → 양쪽 균등 축소 → CER 변화 미미 (~0pp)
```

---

## 5. 결론

### H₁ (대립가설) 기각

`normalize_text()` 정규화 확장으로 CER을 유의미하게 개선할 수 없다.
7개 정규화 항목 중 **0개**가 유의미한 CER 개선을 달성했으며,
4개는 오히려 **8~18pp 악화**를 초래했다.

### H₀ (귀무가설) 채택

CER 40%는 pandoc GT와 PyMuPDF 추출 간의 **실제 구조적 차이**를 반영한다.

| CER 구성 요소 | 비율 | 주요 원인 |
|--------------|------|----------|
| Substitution | 17.4% | 2-column 읽기 순서 차이, 텍스트 정렬 불일치 |
| Insertion | 19.0% | 추출에만 존재하는 콘텐츠 (References, 페이지번호, 저자정보) |
| Deletion | 4.1% | GT에만 존재하는 콘텐츠 (수식, pandoc 아티팩트) |

---

## 6. 시사점 및 후속 연구 방향

H₀ 채택에 따라, CER 개선을 위해서는 정규화가 아닌 **근본적 접근**이 필요하다:

| 트랙 | 접근 방식 | 기대 효과 | 대상 CER 구성 |
|------|----------|----------|--------------|
| **A. GT 재빌드** | pandoc 변환 시 References 포함, 콘텐츠 구성 일치 | Insertion 19% 대폭 감소 | Insertion |
| **B. 평가 재설계** | 섹션별 CER, BLEU/ROUGE 보조 메트릭 도입 | 공정한 비교 기준 확립 | 전체 |
| **C. 추출 개선** | pymupdf4llm, column-aware 정렬 | Substitution 17% 감소 | Substitution |

> **권장**: Track A + B 병행 — GT와 추출의 콘텐츠 범위를 일치시킨 후,
> 섹션별 CER로 본문 추출 품질을 정확히 측정하는 것이 학술적으로 가장 엄밀한 접근이다.
