# PRD Analysis Report

## 분석 대상
- **문서**: `docs/prd/PRD_extraction_quality_improvement.md`
- **버전**: 1.0
- **분석일**: 2026-01-30

## 요약

| 카테고리 | 발견 | Critical | Major | Minor |
|----------|------|----------|-------|-------|
| 완전성 | 3 | 1 | 1 | 1 |
| 실현가능성 | 4 | 2 | 2 | 0 |
| 일관성 | 2 | 1 | 1 | 0 |
| **총계** | **9** | **4** | **4** | **1** |

---

## 상세 분석

### 🔴 Critical (즉시 수정 필요)

#### C-1. FR-001 참고문헌 제거가 CER을 악화시킴

- **위치**: Section 4.2 — FR-001
- **문제**: PRD는 "참고문헌을 양쪽에서 제거하면 CER이 5~15pp 개선"이라고 가정했으나, 실제 시뮬레이션 결과 CER이 **40% → 56%로 악화**됨.
- **근본 원인**:
  - GT에는 "References" 섹션 헤더가 **39개 문서 중 0개**에서 발견됨 → regex가 GT에 매칭되지 않음
  - 추출에서만 References(12K chars) 제거 → 추출이 30K로 짧아지고 GT(37K)보다 7K 부족 → Deletion 폭증
  - `normalize_text()`는 양쪽에 동일 적용되므로, **한쪽에만 존재하는 콘텐츠를 제거하면 길이 불균형이 심화**됨
- **영향**: PRD에서 가장 높은 CER 개선(5~15pp)을 예상한 항목이 오히려 16pp 악화
- **개선안**: FR-001 폐기. 참고문헌 차이는 CER의 구조적 한계이므로 별도 메트릭(섹션별 CER)으로 대응하거나, GT 재빌드 시 참고문헌 섹션을 포함시켜야 함

#### C-2. FR-003 러닝 헤더 감지가 표 셀을 대량 오탐

- **위치**: Section 4.4 — FR-003
- **문제**: "3회 이상 반복 + 5~80자" 조건이 논문 표의 셀 내용을 오탐함
- **실측 데이터**:
  - `test_arxiv_003` (ResNet): "3x3 conv, 256" 26회, "3x3 conv, 512" 18회 반복 → 모두 표 셀
  - `test_arxiv_004` (GPT-3): "Context →" 50회, "Target Completion →" 50회 → 벤치마크 예시
  - `test_arxiv_005` (VGGNet): "conv3-512" 30회 → 아키텍처 표
  - `test_arxiv_001` (Transformer): "perfect", "application" 등 단어 8회 → 번역 예시 표
- **영향**: 본문/표의 유효 콘텐츠를 삭제하여 CER 악화
- **개선안**: 단순 빈도 기반 감지는 사용 불가. 대안:
  1. 페이지 경계(`[Page N]`) 직후 첫 줄만 검사하는 위치 기반 감지
  2. 또는 러닝 헤더 감지 자체를 포기하고, GT/추출 양쪽에 동일하게 존재하므로 CER에 미치는 영향이 제한적임을 수용

#### C-3. PRD의 근본 전제 오류: "측정 노이즈" vs "실제 콘텐츠 차이"

- **위치**: Section 1.1, 전체 접근 방식
- **문제**: PRD는 CER 40%의 주원인을 "측정 노이즈"로 진단하고, normalize_text()에서 노이즈를 제거하면 CER이 개선된다고 가정. 그러나:
  - `normalize_text()`는 **GT와 추출 양쪽에 동일 적용**
  - 한쪽에만 있는 콘텐츠(페이지번호, 이메일, References)를 제거하면, 해당 쪽만 짧아져서 **길이 불균형이 심화**됨
  - 페이지번호만 제거: 40.0% → 39.6% (0.4pp 개선, 거의 무의미)
  - 페이지번호+이메일+arXiv+각주마커 제거: 40.0% → 47.8% (7.8pp **악화**)
  - References 제거: 40.0% → 56.2% (16.2pp **악화**)
- **영향**: PRD의 FR-001~007 전체가 CER 개선에 기여하지 못하거나 악화시킴
- **근본 원인**: CER 40%는 "노이즈"가 아니라 GT와 추출 사이의 **실제 콘텐츠/구조 차이**:
  - GT: pandoc이 LaTeX에서 변환한 마크다운 (인용은 `[@key]`, 참고문헌 리스트 없음, 수식 포함)
  - 추출: PyMuPDF가 PDF에서 추출한 raw text (인용은 `[1]`, References 섹션 있음, 수식은 깨짐)
  - 이 두 소스의 콘텐츠 구성이 근본적으로 다르므로 CER 40%는 정상 범위

#### C-4. CER 목표값 < 20%가 현 아키텍처에서 달성 불가능

- **위치**: Section 7 — Success Metrics
- **문제**: pandoc GT vs PyMuPDF 추출 비교에서 CER < 20%는 구조적으로 달성 불가능
  - Substitution 17.4%만으로 이미 목표 초과 (읽기 순서 + 텍스트 정렬 차이)
  - Insertion 19%는 추출에만 있는 콘텐츠 (저자, References) → 제거 불가 (악화됨)
- **개선안**: 목표를 현실적으로 재설정하거나, 평가 방식 자체를 재설계

### 🟡 Major (구현 전 수정 권장)

#### M-1. FR-002 페이지 번호 제거 효과가 미미

- **위치**: Section 4.3
- **문제**: 시뮬레이션 결과 CER 40.0% → 39.6% (0.4pp 개선). PRD 예상(2~5pp)과 큰 괴리.
- **원인**: 단독 숫자 줄은 1~3자리로 매우 짧아 전체 CER에 기여하는 문자 수가 적음 (29~152줄 × 평균 2자 = 58~304자 vs 전체 37K+)
- **개선안**: 효과가 미미하므로 우선순위 하향 (P0 → P2)

#### M-2. FR-004 이메일 제거가 CER을 악화시킴

- **위치**: Section 4.5
- **문제**: 이메일은 추출에만 있으므로 제거 시 추출만 짧아져서 CER 악화
- **개선안**: FR-004 폐기 또는 재설계

#### M-3. FR-008 CER 상세 저장은 유효하나 별도 구현 가능

- **위치**: Section 4.8
- **문제**: evaluation.json에 cer_detail이 이미 evaluate_results()에서 계산되지만 save_results_to_files()에서 누락됨. 이것은 유효한 요구사항.
- **개선안**: FR-008은 유지. 독립적으로 구현 가능.

#### M-4. 2-column 읽기 순서 문제가 미해결

- **위치**: Section 2.2 — 노이즈 패턴
- **문제**: Substitution 17.4%의 주원인이 2-column 읽기 순서이나, PRD에서 구체적 해결 방안이 없음
  - Non-Goals에 "PyMuPDF 교체 제외"로 명시했으나, 이것이 CER의 최대 개선 포인트
- **개선안**: `page.get_text("blocks")` 기반 column-aware 정렬을 normalize_text() 외부에서 별도 전처리로 검토

### 🟢 Minor (개선 제안)

#### m-1. CER 개선 예상치가 검증되지 않은 추정

- **위치**: Section 3 — Functional Requirements, CER 개선 예상 열
- **문제**: 각 FR의 "CER 개선 예상"이 시뮬레이션 없이 직감으로 작성됨. 실측과 10~20pp 괴리.
- **개선안**: 각 FR을 시뮬레이션하여 실측 기반 수치로 교체

---

## 누락된 요구사항

| ID | 요구사항 | 권장 우선순위 |
|----|---------|--------------|
| NEW-1 | GT 재빌드: pandoc 변환 시 References 섹션 포함하여 양쪽 구조 일치시키기 | P0 |
| NEW-2 | 섹션별 CER 분리 측정: 본문/Abstract/References를 나눠서 평가 | P0 |
| NEW-3 | PyMuPDF `get_text("blocks")` 기반 column-aware 추출 또는 pymupdf4llm 전환 검토 | P1 |
| NEW-4 | CER 외 보조 메트릭 도입: BLEU, ROUGE 등 순서 무관 유사도 | P1 |

---

## 리스크 매트릭스

| 리스크 | 발생 확률 | 영향도 | 대응 방안 |
|--------|----------|--------|----------|
| normalize에서 한쪽만 제거하여 CER 악화 | **확정 (이미 발생)** | 고 | FR-001,004 폐기 |
| 러닝 헤더 감지 오탐으로 표 데이터 삭제 | **확정 (이미 확인)** | 고 | FR-003 재설계 또는 폐기 |
| CER 목표 미달성으로 프로젝트 방향 혼란 | 고 | 고 | 목표 재설정 + 평가 방식 재설계 |
| GT-추출 간 구조적 불일치가 해소 안 됨 | 고 | 고 | GT 재빌드 또는 섹션별 평가 |

---

## 권장 조치

### 즉시 조치 (Critical)

1. ❗ **FR-001 폐기** — 참고문헌 제거는 CER을 16pp 악화시킴
2. ❗ **FR-003 재설계** — 빈도 기반 러닝 헤더 감지는 표 셀 오탐. 위치 기반으로 전환하거나 폐기
3. ❗ **FR-004 폐기** — 이메일 제거도 CER 악화
4. ❗ **PRD 전제 재검토** — "normalize로 CER 개선" 접근 자체가 한계. 대안 전략 필요

### 구현 전 조치 (Major)

1. ⚠️ **NEW-1**: GT 재빌드 시 References 섹션을 포함하는 것이 CER 개선의 가장 효과적 경로
2. ⚠️ **NEW-2**: 섹션별 CER을 도입하여 "본문 CER"과 "전체 CER"을 분리 측정
3. ⚠️ **FR-008 유지**: evaluation.json에 CER 상세 저장은 진단에 필수

### 가능하면 조치 (Minor)

1. 💡 FR-002 (페이지번호 제거): 0.4pp 효과이지만 부작용 없으므로 유지 가능
2. 💡 FR-006, FR-007: 미미한 효과이지만 부작용도 적음

---

## PRD 재작성 방향 제안

현 PRD의 접근("normalize_text 확장으로 CER 개선")은 근본적 한계가 있으므로, 다음 3가지 대안 트랙을 제안:

### 트랙 A: GT 품질 개선 (가장 효과적)
- GT 재빌드 시 pandoc 출력에 References 섹션 포함
- latex_postprocess.py에서 과도한 콘텐츠 제거 방지
- GT와 추출의 콘텐츠 구성을 일치시키면 CER이 자연스럽게 하락

### 트랙 B: 평가 방식 재설계
- 섹션별 CER (Abstract, Body, References 분리)
- BLEU/ROUGE 보조 메트릭으로 순서 무관 유사도 측정
- 현 CER 40%를 "정상 범위"로 재해석하고 보고

### 트랙 C: 추출 품질 실질 개선
- PyMuPDF의 `get_text("blocks")` + column-aware 정렬
- pymupdf4llm 전환 검토 (마크다운 직접 출력)
- 이것이 Substitution 17%를 줄일 유일한 방법

---

## 다음 단계

PRD를 위 분석 결과를 반영하여 재작성한 후, `/implement`로 구현을 시작하세요.

> 💡 **가장 빠른 승리**: FR-008 (CER 상세 저장)만 먼저 구현하고, GT 재빌드 또는 평가 방식 재설계를 별도 PRD로 진행하는 것을 권장합니다.
